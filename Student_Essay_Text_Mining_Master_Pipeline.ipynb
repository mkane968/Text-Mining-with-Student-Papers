{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuaU3p2wpq0mzz3217Fpw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/Student_Essay_Text_Mining_Master_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master Pipeline: Text Mining Student Essays\n",
        "\n",
        "This notebook lets users clean, explore and analyze student academic essays.\n",
        "\n",
        "First, any given corpus of student essays can be merged into single dataframe and  associated with their metadata in a master dataframe.\n",
        "\n",
        "Then, basic cleaning and enrichment can be performed on the corpus. \n",
        "\n",
        "Using a combination of natural language processing techniques, each essay can then be searched for **key words and phrases** associated with students' **uptake of certain first-year writing program outcomes.** Examples include: \n",
        "*   `Rhetorical analysis terms like \"pathos\" , \"ethos\" , \"logos\"` &rarr; *To learn to employ rhetorical terms and strategies and strengthen your ability to analyze rhetorical techniques in published essays and visual texts.*\n",
        "*   `Regular expressions associated with in-text citations` &rarr; *To learn to employ academic evidence*\n",
        "*   `Terms from stance word dictionary` &rarr; *To develop competent academic arguments over multiple drafts*\n",
        "\n",
        "*Outcomes from ENG 701 Syllabus, Temple University First-Year Writing Program, Revised June 2022*\n",
        "\n",
        "Once texts have been searched for these outcomes, their surrounding context can be retrieved and stored for further analysis. Since each text is associated with its grade metadata, researchers can investigate trends in language patterns and their differences across high (A), mid-range (B-C) and low-scoring (D-F) essays. This can be done using natural-language processing tools in this notebook as well as [the DocuScope tool.](https://www.cmu.edu/dietrich/english/research-and-publications/docuscope.html)\n",
        "\n",
        "\n",
        "This code can support program instructors who seek to gauge patterns of student performance on valued writing program outcomes, articulate those outcomes to instructors, and identify areas where outcomes can be refined, reshaped, or expanded to better capture what is valued about studentsâ€™ languaging practices. \n"
      ],
      "metadata": {
        "id": "KBM4YXi32P7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages"
      ],
      "metadata": {
        "id": "iKFBdIXp9cjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_La5Ur32KKX",
        "outputId": "2c8464bc-334c-49ae-e7af-f3c546d7da85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "#Install glob\n",
        "import glob \n",
        "\n",
        "#Install pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Install numpy\n",
        "import numpy as np\n",
        "\n",
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "\n",
        "#Import matplotlib for visualizations\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Add Student Essays and Metadata to DataFrame"
      ],
      "metadata": {
        "id": "LmRdlSk1-Q8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload all student essays from local folder and add to dataframe"
      ],
      "metadata": {
        "id": "07_EUmsB-u8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqzr4eGV2PYi",
        "outputId": "a874fb3d-3550-4bce-d1c7-58f340ca97d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files to upload from local machine\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "OshZ_q2J-X85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essays = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "essays.head()"
      ],
      "metadata": {
        "id": "Aaq5zqob-ci5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "essays = essays.reset_index()\n",
        "essays.columns = [\"ID\", \"Text\"]\n",
        "essays\n"
      ],
      "metadata": {
        "id": "okyWuNHa-jGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "essays['Text'] = essays['Text'].apply(lambda x: x.decode('utf-8'))\n",
        "essays.head()"
      ],
      "metadata": {
        "id": "CHggNO8R-3-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newline characters\n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "essays"
      ],
      "metadata": {
        "id": "UNWKKEGI_IV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove identifying information (student/instructor names) from each paper\n",
        "This isn't perfect -- headers are not standardized across all papers, sometimes students end with prof name or other info, some student names still visible if referenced in papers themselves.\n",
        "\n"
      ],
      "metadata": {
        "id": "BQGCuUWO-6JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove identifying information from ID\n",
        "#Remove any occurences of \"LATE_\" from dataset (otherwise will skew ID cleaning)\n",
        "essays['ID'] = essays['ID'].str.replace(r'LATE_', '', regex=True) \n",
        "\n",
        "#Split book on first underscore (_) in ID, keep only text in between first and second underscore (ID number)\n",
        "start = essays[\"ID\"].str.split(\"_\", expand = True)\n",
        "essays['ID'] = start[1]\n",
        "essays['ID'] = essays['ID'].astype(int)\n",
        "essays"
      ],
      "metadata": {
        "id": "nV1nTQCo_Cht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove headers containing student name, instructor name, course name and date\n",
        "#Split text on 2022 (will likely be last value in headers) and add all contents before to new column\n",
        "headers = essays[\"Text\"].str.split(\"22\", 1, expand = True)\n",
        "essays['Header'] = headers[0]\n",
        "print(essays['Header'])\n",
        "\n",
        "#Add 2022 back to header column\n",
        "essays['Header'] = essays['Header'] + '22'\n",
        "essays['Header'][0]"
      ],
      "metadata": {
        "id": "KphRqehh_LXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove any occurences of the header from the rest of the text in each cell (should be at top of each essay in portfolio)\n",
        "essays['Text_NoHeaders'] = essays.apply(lambda row : row['Text'].replace(str(row['Header']), ''), axis=1)\n",
        "essays['Text_NoHeaders']"
      ],
      "metadata": {
        "id": "JHTy7y58_MAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove old text and header columns from dataframe \n",
        "essays = essays.drop(columns=['Text', 'Header'])\n",
        "essays\n"
      ],
      "metadata": {
        "id": "rsiDDNOQ_Ow_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload essay grades and additional metadata to second dataframe"
      ],
      "metadata": {
        "id": "I22itikk_QwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload csvs with essay metadata\n",
        "uploaded_grades = files.upload()"
      ],
      "metadata": {
        "id": "SzgvrSb6_gnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link to path where csv files are stored in drive\n",
        "local_path = r'/content'\n",
        "\n",
        "#Create variable to store all csvs in path\n",
        "filenames = glob.glob(local_path + \"/*.csv\")\n",
        "\n",
        "#Create df list for all csvs\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]\n",
        "\n",
        "# Concatenate all data into one DataFrame\n",
        "metadata = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "#Change data to string (for further cleaning)\n",
        "metadata.astype(str)\n",
        "\n",
        "metadata"
      ],
      "metadata": {
        "id": "ntoCPrN2_xAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop header rows(Points Possible) and test student rows (Student, Test)\n",
        "metadata = metadata[metadata['Student'].str.contains('Points Possible|Student, Test')==False]\n",
        "metadata\n"
      ],
      "metadata": {
        "id": "Qgo-19g4_6AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get all column names\n",
        "for col in metadata.columns:\n",
        "    print(col)\n",
        "\n",
        "#Choose which rows to keep (ID, section and final portfolios with #s after chosen here)\n",
        "metadata = metadata[['ID', 'Section', \"Final Portfolio (1689777)\", \"Final Portfolio (1676963)\"]]\n",
        "metadata"
      ],
      "metadata": {
        "id": "a_VvMfTU_7u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace all NaN values with 0 \n",
        "metadata = metadata.replace(np.nan, 0)"
      ],
      "metadata": {
        "id": "180g-ohp_8Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new final portfolio column with all values\n",
        "#Not sure how this will work with more than two dataframes\n",
        "metadata['Portfolio Score'] = metadata['Final Portfolio (1689777)'] + metadata['Final Portfolio (1676963)']\n",
        "metadata\n"
      ],
      "metadata": {
        "id": "bNzvBqD2__TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop grade columns for individual classes\n",
        "clean_metadata = metadata[['ID', 'Section', \"Portfolio Score\"]]\n",
        "clean_metadata"
      ],
      "metadata": {
        "id": "iGJnhNdZAB9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop decimal from ID (inconsistent with ID in essay dataframe)\n",
        "clean_metadata['ID'] = clean_metadata['ID'].astype(int)\n",
        "\n",
        "#Check cleaned DF one more time\n",
        "clean_metadata\n"
      ],
      "metadata": {
        "id": "Bz_wEeL0ACdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge essays and grade metadata into one dataframe"
      ],
      "metadata": {
        "id": "pMhaQbM2AEMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge metadata and cleaned essays into new dataframe\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "essays_grades_df = clean_metadata.merge(essays,on='ID')\n",
        "essays_grades_df"
      ],
      "metadata": {
        "id": "nwAjifpvALsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "essays_grades_df.to_csv('essays_grades.csv') \n",
        "files.download('essays_grades.csv')"
      ],
      "metadata": {
        "id": "GYmo80TOAPHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clean and Enrich Student Essays "
      ],
      "metadata": {
        "id": "JKNvuCc4ApQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning: Lowercasing, Punctuation Removal, and Stopword Removal"
      ],
      "metadata": {
        "id": "fuZQOgu0Bi15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "essays_grades_df['Text'] = essays_grades_df['LC_Text'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "essays_grades_df['NoPunct_Text'] = essays_grades_df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "essays_grades_df['NoPunct_Text'] = essays_grades_df['NoPunct_Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "essays_grades_df['no_stops'] = essays_grades_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Check output\n",
        "essays_grades_df.head()"
      ],
      "metadata": {
        "id": "9_Ux-p8QAvrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrichment: Lemmatization, Part-of-Speech Tagging, and Named Entity Recognition"
      ],
      "metadata": {
        "id": "iw0sv-dNCdtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make dataframe for enriched data\n",
        "enriched_essays_grades_df = essays_grades_df\n",
        "\n",
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_essays_grades_df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_essays_grades_df['lemma_list'] = lemma_list\n",
        "enriched_essays_grades_df.head()\n"
      ],
      "metadata": {
        "id": "Z7JHQTwNBbOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_essays_grades_df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_essays_grades_df['pos_list'] = pos_list\n",
        "\n",
        "#Check pos tags\n",
        "enriched_essays_grades_df.head()\n"
      ],
      "metadata": {
        "id": "liJDwvyDDMON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for single doc and visualize\n",
        "doc = nlp(enriched_essays_grades_df.Text_NoHeaders[0]) \n",
        "print(doc)\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "mqXDfgxGBxIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_essays_grades_df.Text.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "enriched_essays_grades_df['ent_list'] = ent_list\n",
        "\n",
        "#Check named entities\n",
        "enriched_essays_grades_df.head()\n"
      ],
      "metadata": {
        "id": "F6eCuPFeDZOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(enriched_essays_grades_df.Text[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "duIikij4DZRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download enriched df\n",
        "enriched_essays_grades_df.to_csv('essays_grades_enriched.csv') \n",
        "files.download('essays_grades_enriched.csv')"
      ],
      "metadata": {
        "id": "Raa-7f9EEd_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interlude: Some preliminary analyses of essay length, part-of-speech counts, and named entities\n",
        "\n",
        "(Work in progress)"
      ],
      "metadata": {
        "id": "5u2tynfpClYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word count of each text\n",
        "essays_grades_df['Length'] = essays_grades_df['Text'].apply(lambda x: len(x))\n",
        "essays_grades_df.head()"
      ],
      "metadata": {
        "id": "3ZBGq1AdCu-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph portfolio grade by length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "essays_grades_df = essays_grades_df.sort_values(by=['Portfolio Score'], ascending=True)\n",
        "\n",
        "essays_grades_df.plot(kind='bar',x='Portfolio Score',y='Length')"
      ],
      "metadata": {
        "id": "Vlx-iw70Cw-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Section Texts Based on Outcomes-Related Keywords"
      ],
      "metadata": {
        "id": "P8uBilltBxpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Sections of Each Essay Containing Rhetorical Analysis Terms\n",
        "**Related Outcome:** To learn to employ rhetorical terms and strategies and strengthen your ability to analyze rhetorical techniques in published essays and visual texts.\n",
        "\n"
      ],
      "metadata": {
        "id": "tbrF-oWtDpIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only need one version of the cleaned text for this essay; will choose stopwords one here\n",
        "df_rhetorical = essays_grades_df.drop([\"LC_Text\", \"Text\", \"NoPunct_Text\"], axis=1)\n",
        "df_rhetorical.head()"
      ],
      "metadata": {
        "id": "1MbDKMScBxLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up column for score plus ID\n",
        "df_rhetorical['ID + Score'] = df_rhetorical['ID'].astype(str) + '_' + df_rhetorical['Final Portfolio'].astype(str)\n",
        "\n",
        "#Count number of occurences of rhetorical terms in each paper\n",
        "pathos_counts = df_rhetorical['no_stops'].str.count('pathos')\n",
        "ethos_counts = df_rhetorical['no_stops'].str.count('ethos')\n",
        "logos_counts = df_rhetorical['no_stops'].str.count('logos')\n",
        "\n",
        "df_rhetorical.head()"
      ],
      "metadata": {
        "id": "rGJdWJpIEUvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph number of pathos, ethos and logos mentions across essays\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Pathos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Pathos_Counts\"]),\n",
        "    go.Bar(name='Ethos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Ethos_Counts\"]),\n",
        "    go.Bar(name='Logos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Logos_Counts\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "w6uab2kCEWdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use concordancing to get context around each rhetorical term\n",
        "\n",
        "def concordance(ci, word, width=400, lines=25):\n",
        "    \"\"\"\n",
        "    Rewrite of nltk.text.ConcordanceIndex.print_concordance that returns results\n",
        "    instead of printing them. \n",
        "\n",
        "    See:\n",
        "    http://www.nltk.org/api/nltk.html#nltk.text.ConcordanceIndex.print_concordance\n",
        "    \"\"\"\n",
        "    half_width = (width - len(word) - 2) // 2\n",
        "    context = width // 2 # approx number of words of context\n",
        "\n",
        "    results = []\n",
        "    offsets = ci.offsets(word)\n",
        "    if offsets:\n",
        "        lines = min(lines, len(offsets))\n",
        "        for i in offsets:\n",
        "            if lines <= 0:\n",
        "                break\n",
        "            left = (' ' * half_width +\n",
        "                    ' '.join(ci._tokens[i-context:i]))\n",
        "            right = ' '.join(ci._tokens[i+1:i+context])\n",
        "            left = left[-half_width:]\n",
        "            right = right[:half_width]\n",
        "            results.append('%s %s %s' % (left, ci._tokens[i], right))\n",
        "            lines -= 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "FS-uIQQ2EYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get context around each instance of pathos in each essay and append to dataframe\n",
        "pathos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'pathos')\n",
        "  pathos_results.append(results)\n",
        "\n",
        "pathos_df = pd.DataFrame(pathos_results)\n",
        "\n",
        "\n",
        "pathos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "pathos_df.head()\n"
      ],
      "metadata": {
        "id": "0R_kB3x2EYyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Associate each use of pathos with score and reset index, tidy column names\n",
        "pathos_df = pathos_df.set_index('ID_Score')\n",
        "pathos_clean = pathos_df.stack().reset_index()\n",
        "\n",
        "pathos_clean.columns = [\"ID_Score\",\"Pathos_Count\",\"Pathos_Context\"]\n",
        "\n",
        "pathos_clean.head()\n"
      ],
      "metadata": {
        "id": "2Gv1_QowE2Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the above steps for ethos concordancing\n",
        "\n",
        "#Get context around each instance of ethos in each essay and append to dataframe\n",
        "ethos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'ethos')\n",
        "  ethos_results.append(results)\n",
        "\n",
        "ethos_df = pd.DataFrame(ethos_results)\n",
        "\n",
        "\n",
        "ethos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "#Associate each instance with score and reset index, tidy column names\n",
        "ethos_df = ethos_df.set_index('ID_Score')\n",
        "ethos_clean = ethos_df.stack().reset_index()\n",
        "\n",
        "ethos_clean.columns = [\"ID_Score\",\"Ethos_Count\",\"Ethos_Context\"]\n",
        "\n",
        "ethos_clean.head()\n"
      ],
      "metadata": {
        "id": "XsHIjxzHE_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the above steps for logos concordancing\n",
        "\n",
        "#Get context around each instance of ethos in each essay and append to dataframe\n",
        "logos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'logos')\n",
        "  logos_results.append(results)\n",
        "\n",
        "logos_df = pd.DataFrame(logos_results)\n",
        "\n",
        "\n",
        "logos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "#Associate each instance with score and reset index, tidy column names\n",
        "logos_df = logos_df.set_index('ID_Score')\n",
        "logos_clean = logos_df.stack().reset_index()\n",
        "\n",
        "logos_clean.columns = [\"ID_Score\",\"Logos_Count\",\"Logos_Context\"]\n",
        "\n",
        "logos_clean.head()"
      ],
      "metadata": {
        "id": "EMF09BvQFBKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine into single dataframe\n",
        "import functools as ft\n",
        "rhetorical_dfs = [logos_clean, pathos_clean, ethos_clean]\n",
        "rhetorical_df_final = ft.reduce(lambda left, right: pd.merge(left, right, on='ID_Score'), rhetorical_dfs)\n",
        "\n",
        "#Clean by removing duplicate values of each (replicated during merge)\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Pathos_Context'].duplicated(), 'Pathos_Context'] = 'None'\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Logos_Context'].duplicated(), 'Logos_Context'] = 'None'\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Ethos_Context'].duplicated(), 'Ethos_Context'] = 'None'\n",
        "\n",
        "rhetorical_df_final\n"
      ],
      "metadata": {
        "id": "vgIjRxvqFFNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download file to csv\n",
        "rhetorical_df_final.to_csv('rhetorical_context_df.csv', encoding = 'utf-8-sig') \n",
        "files.download('rhetorical_context_df.csv')"
      ],
      "metadata": {
        "id": "jC184vLXFjwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Sections of Each Essay Containing Citations\n",
        "**Related Outcome:** To learn to employ academic evidence\n",
        "\n",
        "This section DOES NOT work yet; going to adapt this code: https://levelup.gitconnected.com/count-citations-in-a-word-document-with-python-and-regular-expressions-d068218c50b9"
      ],
      "metadata": {
        "id": "eCfaS8EoFGTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://ideone.com/IqZvxm\n",
        "\n",
        "import re\n",
        "pattern = r'\\(([^\"\\)]*|\\bAnonymous\\b|\"[^\"\\)]*\")(, )([\\d]+|n\\.d\\.|[\\d]+[\\w])\\)'\n",
        "num_replaces = 100000000\n",
        "\n",
        "\n",
        "# Try to find citation matches (returned as an iterator of matches)\n",
        "citation_results = []\n",
        "for text in essays_grades_df['Text_NoHeaders']:\n",
        "  results = re.finditer(pattern, text)\n",
        "  citation_results.append(results)\n",
        "\n",
        "citation_results"
      ],
      "metadata": {
        "id": "BSGhICbxFNcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Sections of Each Essay Containing Stance Words\n",
        "** Related Outcome:** To develop competent academic arguments\n",
        "\n",
        "This section DOES NOT work yet; going to adapt methods from:\n",
        "\n",
        "*   https://www.sciencedirect.com/science/article/abs/pii/S147515851730005X\n",
        "*   https://journals-sagepub-com.libproxy.temple.edu/doi/full/10.1177/0741088314527055\n"
      ],
      "metadata": {
        "id": "mh4pM60WFWVl"
      }
    }
  ]
}
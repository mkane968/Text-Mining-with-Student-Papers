{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/rvxjTAC9IvS+lTZct1ge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/notebooks/Text_Mining_Student_Papers_12_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Mining Student Papers: A Computational Exploration\n",
        "\n",
        "By Megan Kane\n",
        "\n",
        "Drafted as part of the Fall 2022 Cultural Analytics Practicum at Temple University.\n",
        "\n"
      ],
      "metadata": {
        "id": "DSS87JYOvvFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages"
      ],
      "metadata": {
        "id": "zDd6JVT4cxbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIv9gPl0cBQo"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "#Install glob\n",
        "import glob \n",
        "\n",
        "#Install pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Install numpy\n",
        "import numpy as np\n",
        "\n",
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from  nltk.text import ConcordanceIndex\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Import matplotlib for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import logging  # Setting up the loggings to monitor gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Student Essays and Metadata"
      ],
      "metadata": {
        "id": "61xG-OZIdC0u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_EUmsB-u8i"
      },
      "source": [
        "###Import Student Essays and Add to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqzr4eGV2PYi"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OshZ_q2J-X85"
      },
      "outputs": [],
      "source": [
        "#Add files to upload from local machine\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aaq5zqob-ci5"
      },
      "outputs": [],
      "source": [
        "#Put essays into dataframe\n",
        "essays = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "\n",
        "#Reset index and add column names to make wrangling easier\n",
        "essays = essays.reset_index()\n",
        "essays.columns = [\"ID\", \"Text\"]\n",
        "\n",
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "essays['Text'] = essays['Text'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "#Remove newline characters and put in new column \n",
        "essays['Text_Newlines'] = essays['Text']\n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "essays.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add column without identifying information from each paper ID (instructor/student names) "
      ],
      "metadata": {
        "id": "Jm37ef7fdwUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNWKKEGI_IV2"
      },
      "outputs": [],
      "source": [
        "#Remove identifying information from ID\n",
        "#Remove any occurences of \"LATE_\" from dataset (otherwise will skew ID cleaning)\n",
        "essays['ID'] = essays['ID'].str.replace(r'LATE_', '', regex=True) \n",
        "\n",
        "#Split book on first underscore (_) in ID, keep only text in between first and second underscore (ID number)\n",
        "start = essays[\"ID\"].str.split(\"_\", expand = True)\n",
        "essays['ID'] = start[1]\n",
        "essays['ID'] = essays['ID'].astype(int)\n",
        "essays"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(essays)"
      ],
      "metadata": {
        "id": "otkAjDcw19n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I22itikk_QwV"
      },
      "source": [
        "### Import grades and additional metadata to second dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzgvrSb6_gnN"
      },
      "outputs": [],
      "source": [
        "#Upload csvs with essay metadata\n",
        "uploaded_grades = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntoCPrN2_xAt"
      },
      "outputs": [],
      "source": [
        "#Link to path where csv files are stored in drive\n",
        "local_path = r'/content'\n",
        "\n",
        "#Create variable to store all csvs in path\n",
        "filenames = glob.glob(local_path + \"/*.csv\")\n",
        "\n",
        "#Create df list for all csvs\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]\n",
        "\n",
        "len(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all data into one DataFrame\n",
        "metadata = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "#Change data to string (for further cleaning)\n",
        "metadata.astype(str)\n",
        "\n",
        "metadata.head()"
      ],
      "metadata": {
        "id": "uBIcqCFk2PQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgo-19g4_6AN"
      },
      "outputs": [],
      "source": [
        "#Drop header rows(Points Possible) and test student rows (Student, Test)\n",
        "metadata = metadata[metadata['Student'].str.contains('Points Possible|Student, Test')==False]\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep only relevant metadata (ID, Section, Final Portfolio Scores)\n",
        "clean_metadata = metadata[['ID'] + ['Section'] + list(metadata.loc[:, metadata.columns.str.startswith('Final Portfolio (')])]\n",
        "\n",
        "\n",
        "#Change columns to float as needed (check with df.dtypes())\n",
        "clean_metadata[\"Final Portfolio (1Score)\"] = pd.to_numeric(clean_metadata[\"Final Portfolio (1Score)\"], downcast=\"float\")\n",
        "clean_metadata[\"Final Portfolio (Score)\"] = pd.to_numeric(clean_metadata[\"Final Portfolio (Score)\"], downcast=\"float\")\n",
        "#Want other metadata? Check the columns\n",
        "#Get all column names \n",
        "#for col in metadata.columns:\n",
        "   # print(col)"
      ],
      "metadata": {
        "id": "Et65_490s5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "180g-ohp_8Tq"
      },
      "outputs": [],
      "source": [
        "#Replace all NaN values with 0 \n",
        "clean_metadata = clean_metadata.replace(np.nan, 0)\n",
        "clean_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzvBqD2__TA"
      },
      "outputs": [],
      "source": [
        "#Create new final portfolio column with all values\n",
        "#Add values of each column together; values except correct grade will be zero\n",
        "score_counts = clean_metadata.columns[2:]\n",
        "clean_metadata['Portfolio_Score'] = clean_metadata[score_counts].sum(axis=1)\n",
        "clean_metadata['Portfolio_Score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGJnhNdZAB9-"
      },
      "outputs": [],
      "source": [
        "#Drop grade columns for individual classes\n",
        "clean_metadata = clean_metadata[['ID', 'Section', \"Portfolio_Score\"]]\n",
        "\n",
        "#Round scores to nearest integer\n",
        "clean_metadata.Portfolio_Score = clean_metadata.Portfolio_Score.round()\n",
        "clean_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz_wEeL0ACdW"
      },
      "outputs": [],
      "source": [
        "#Drop decimal from ID (inconsistent with ID in essay dataframe)\n",
        "clean_metadata['ID'] = clean_metadata['ID'].astype(int)\n",
        "\n",
        "#Check cleaned DF one more time\n",
        "clean_metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMhaQbM2AEMC"
      },
      "source": [
        "### Merge essays and grade metadata into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwAjifpvALsP"
      },
      "outputs": [],
      "source": [
        "#Merge metadata and cleaned essays into new dataframe\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "essays_grades_master = clean_metadata.merge(essays,on='ID')\n",
        "\n",
        "#Print dataframe\n",
        "essays_grades_master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort dataframe by grades\n",
        "essays_grades_master.sort_values(by=['Portfolio_Score'], inplace = True)\n",
        "essays_grades_master.head()"
      ],
      "metadata": {
        "id": "eLM2Jz5Gd05R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYmo80TOAPHf"
      },
      "outputs": [],
      "source": [
        "#Save new df to csv and download\n",
        "essays_grades_master.to_csv('essays_grades_master.csv') \n",
        "files.download('essays_grades_master.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save and download de-identified essays for future analysis\n",
        "#Add each text to a new list called paragraph_context\n",
        "deidentified_texts = []\n",
        "for row in essays_grades_master['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    deidentified_texts.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in essays_grades_master['ID'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir deidentified_texts\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in deidentified_texts:\n",
        "  f = open(\"deidentified_texts/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "\n",
        "#Zip text files in folder\n",
        "!zip -r deidentified_texts.zip deidentified_texts\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('deidentified_texts.zip')"
      ],
      "metadata": {
        "id": "bQUaYD_Zop3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clean Data"
      ],
      "metadata": {
        "id": "qIYZO-LW2sGO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZQOgu0Bi15"
      },
      "source": [
        "### Basic Cleaning with NLTK\n",
        "####Lowercasing, Punctuation Removal, and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Ux-p8QAvrZ"
      },
      "outputs": [],
      "source": [
        "#Rename dataframe\n",
        "clean_essay_grades_df = essays_grades_master\n",
        "clean_essay_grades_df.rename(columns = {\"Text_NoHeaders\": \"Text\"}, inplace = True)\n",
        "\n",
        "#Lowercase all words\n",
        "clean_essay_grades_df['Lower_Text'] = clean_essay_grades_df['Text'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['Lower_Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['NoPunct_Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "clean_essay_grades_df['NoStops_Text'] = clean_essay_grades_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Check output\n",
        "clean_essay_grades_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Text into Paragraphs"
      ],
      "metadata": {
        "id": "3U1COuZkFQqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only need one newlines version here\n",
        "paragraphs_df = clean_essay_grades_df[['Portfolio_Score','ID', 'Text_Newlines']].copy()\n",
        "\n",
        "#Add ID and score in one column\n",
        "paragraphs_df['Score_ID'] = 'Score: ' + paragraphs_df['Portfolio_Score'].astype(str) + ', ID: ' + paragraphs_df['ID'].astype(str)\n",
        "\n",
        "#Check new df\n",
        "paragraphs_df.head()\n"
      ],
      "metadata": {
        "id": "PjxeIWG2HsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of paragraphs in each text\n",
        "paragraph_counts = paragraphs_df['Text_Newlines'].str.count(r'\\n')\n",
        "paragraph_counts\n",
        "\n",
        "#Append paragraphs counts to dataframe\n",
        "paragraphs_df[\"Paragraph_Counts\"] = paragraph_counts\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "AKA5Xxo7Dovo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new paragraph starts \n",
        "new = paragraphs_df[\"Text_Newlines\"].str.split(r'\\n', expand = True).set_index(paragraphs_df['Score_ID'])\n",
        "\n",
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "paragraphs_df = new.stack().reset_index()\n",
        "paragraphs_df.columns = [\"Score_ID\", \"Paragraph\", \"Text\"]\n",
        "\n",
        "#Split score and ID back to own columns\n",
        "paragraphs_df[['Score','ID']] = paragraphs_df.Score_ID.str.split(\", \",expand=True)\n",
        "paragraphs_df['Score'] = paragraphs_df['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "paragraphs_df['ID'] = paragraphs_df['ID'].map(lambda x: x.lstrip('ID: '))\n",
        "paragraphs_df['ID_Paragraph'] = paragraphs_df['ID'].astype(str) + '_' + paragraphs_df['Paragraph'].astype(str)\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "m-B7rbmwGv34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Clean paragraphs\n",
        "##Filter out paragraphs with 5 or less words (headers)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.split().str.len().lt(10)]\n",
        "\n",
        "## Filter out paragraphs containing \"http://\", \"doi:\" , \"https://\" and \"://www\" (Works Cited citations)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"http://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"https://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"://www\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"www.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\".com/\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"Vol.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"doi:\")]\n",
        "\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "rTz6Y4cyGv6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download to clean further\n",
        "#paragraphs_df.to_csv('paragraphs.csv') \n",
        "#files.download('paragraphs.csv')"
      ],
      "metadata": {
        "id": "1RuBKJo9Gv8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each paragraph as a txt file\n",
        "#Add each text to a new list called paragraphs\n",
        "paragraphs = []\n",
        "for row in paragraphs_df['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in paragraphs_df['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in paragraphs:\n",
        "  f = open(\"paragraphs/\" + filenames[n] +  '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r paragraphs.zip paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('paragraphs.zip')"
      ],
      "metadata": {
        "id": "ymIHF5KuF2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Identify Keywords in Context"
      ],
      "metadata": {
        "id": "ahLh1N1pB12b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outcome 1: Extracting Rhetorical Analysis Terms and Context"
      ],
      "metadata": {
        "id": "zTx8vs00B6bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Set up new dataframe for keyword frequency counts\n",
        "rhetorical_keywords_paragraphs_df = paragraphs_df.copy()\n",
        "\n",
        "#Count number of occurences of rhetorical terms in each paper\n",
        "pathos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('pathos')\n",
        "ethos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('ethos')\n",
        "logos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('logos')\n",
        "\n",
        "#Append each count to the dataframe\n",
        "rhetorical_keywords_paragraphs_df['Pathos_Counts'] = pathos_counts\n",
        "rhetorical_keywords_paragraphs_df[\"Ethos_Counts\"] = ethos_counts\n",
        "rhetorical_keywords_paragraphs_df[\"Logos_Counts\"] = logos_counts\n",
        "\n",
        "#Get summ of all term usages\n",
        "rhetorical_terms = ['Pathos_Counts', 'Ethos_Counts', 'Logos_Counts']\n",
        "rhetorical_keywords_paragraphs_df['Sum_Terms'] = rhetorical_keywords_paragraphs_df[rhetorical_terms].sum(axis=1)\n",
        "\n",
        "#Split score and ID back to own columns\n",
        "rhetorical_keywords_paragraphs_df[['Score','ID']] = rhetorical_keywords_paragraphs_df.Score_ID.str.split(\", \",expand=True)\n",
        "rhetorical_keywords_paragraphs_df['Score'] = rhetorical_keywords_paragraphs_df['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "rhetorical_keywords_paragraphs_df['ID'] = rhetorical_keywords_paragraphs_df['Score'].map(lambda x: x.lstrip('ID: '))\n",
        "\n",
        "rhetorical_keywords_paragraphs_df"
      ],
      "metadata": {
        "id": "4V7oWhKpSNQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all rows with no rhetorical terms\n",
        "rhetorical_keywords_paragraphs_df_no_blanks = rhetorical_keywords_paragraphs_df[rhetorical_keywords_paragraphs_df.Sum_Terms > 0]\n",
        "rhetorical_keywords_paragraphs_df_no_blanks"
      ],
      "metadata": {
        "id": "lXOWxEyISjkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "rhetorical_keywords_paragraphs_df_no_blanks.to_csv('rhetorical_keywords_paragraphs_df_no_blanks.csv') \n",
        "files.download('rhetorical_keywords_paragraphs_df_no_blanks.csv')"
      ],
      "metadata": {
        "id": "RLCLHX0wWO-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each rhetorical analysis paragraph as a txt file\n",
        "#Add each text to a new list called rhetorical_paragraphs\n",
        "rhetorical_paragraphs = []\n",
        "for row in rhetorical_keywords_paragraphs_df_no_blanks['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    rhetorical_paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in rhetorical_keywords_paragraphs_df_no_blanks['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir rhetorical_paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in rhetorical_paragraphs:\n",
        "  f = open(\"rhetorical_paragraphs/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r rhetorical_paragraphs.zip rhetorical_paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('rhetorical_paragraphs.zip')"
      ],
      "metadata": {
        "id": "hvFp5DsNCnIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this text mining, we have two new data sets to analyze: \n",
        "\n",
        "\n",
        "*   `rhetorical_keywords_paragraphs_df_no_blanks.csv`: A CSV file containing each paragraph where rhetorical terminology was used, along with relevant metadata (can be used for close-reading, frequency and regression analysis, PCA)\n",
        "*  `rhetorical_paragraphs.zip`: A zip file containing plain txt versions of each paragraph where rhetorical terminology was used (can be used for close-reading, DocuScope analysis, topic modeling, and/or other types of corpus analysis)\n",
        "\n",
        "\n",
        "We can also go back and extract other terms, such as synonyms, which may aid later comparative analysis. \n"
      ],
      "metadata": {
        "id": "_W58h_OIDelH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Set up new dataframe for synonym frequency counts in paragraphs\n",
        "rhetorical_synonym_df = rhetorical_keywords_paragraphs_df_no_blanks.copy()"
      ],
      "metadata": {
        "id": "W8RClP8A1AjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rhetorical_synonym_df['Pathos_Synonyms'] = rhetorical_synonym_df['Text'].str.count('experience|feel|stories|story|understand|compassion|passion|anecdote|sad|anger|sympathy|sympathetic|empathy|pity|fear*')\n",
        "rhetorical_synonym_df['Logos_Synonyms'] = rhetorical_synonym_df['Text'].str.count('logic|logical|reason|reasoning|statistic|statistics|fact|facts|common sense|evidence')\n",
        "rhetorical_synonym_df['Ethos_Synonyms'] = rhetorical_synonym_df['Text'].str.count('credible|credibility|authority|ethic|ethical|reliable|fair')\n",
        "rhetorical_synonym_df['Rhetorical_Vocab'] = rhetorical_synonym_df['Text'].str.count('audience|reader|context|situation|rhetorical|element|device|appeal|effective|argue|argument')\n",
        "rhetorical_synonym_df\n"
      ],
      "metadata": {
        "id": "M7Korfv30hk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get summ of all term usages\n",
        "pathos_terms = ['Pathos_Counts', 'Pathos_Synonyms']\n",
        "rhetorical_synonym_df['Sum_Pathos_Terms'] = rhetorical_synonym_df[pathos_terms].sum(axis=1)\n",
        "\n",
        "logos_terms = ['Logos_Counts', 'Logos_Synonyms']\n",
        "rhetorical_synonym_df['Sum_Logos_Terms'] = rhetorical_synonym_df[logos_terms].sum(axis=1)\n",
        "\n",
        "ethos_terms = ['Ethos_Counts', 'Ethos_Synonyms']\n",
        "rhetorical_synonym_df['Sum_Ethos_Terms'] = rhetorical_synonym_df[ethos_terms].sum(axis=1)\n",
        "rhetorical_synonym_df\n",
        "\n",
        "\n",
        "#Get sum of all term usages\n",
        "all_terms = ['Sum_Pathos_Terms', 'Sum_Ethos_Terms', 'Sum_Ethos_Terms', 'Rhetorical_Vocab']\n",
        "rhetorical_synonym_df['Sum_All_Terms'] = rhetorical_synonym_df[all_terms].sum(axis=1)\n",
        "rhetorical_synonym_df"
      ],
      "metadata": {
        "id": "l2XRCPrezJUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outcome 2: Extracting Citation Practices and Context"
      ],
      "metadata": {
        "id": "inP4XLiwEe_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get any text inside parentheticals and count of parentheticals and append to dataframe\n",
        "#https://stackoverflow.com/questions/24696715/regex-for-match-parentheses-in-python\n",
        "parentheticals = r'(?<=\\().*?(?=\\))'\n",
        "\n",
        "#Add new list for parenthetical citations\n",
        "parenthetical_matches = []\n",
        "parenthetical_counts = []\n",
        "\n",
        "#Find all occurences of parenthetical citations in each paragraph of each text\n",
        "citation_df = paragraphs_df.copy()\n",
        "for text in citation_df['Text']:\n",
        "  matches = re.findall(parentheticals, text)\n",
        "  parenthetical_matches.append(matches)\n",
        "  parenthetical_counts.append(len(matches))\n",
        "\n",
        "#Make new column counting all appearances of parentheticals\n",
        "citation_df[\"Parentheticals\"] = parenthetical_matches\n",
        "citation_df['Parenthetical_Counts'] = parenthetical_counts\n",
        "\n",
        "citation_df\n"
      ],
      "metadata": {
        "id": "UFhkaiB6EsL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all rows with no parenthetical terms\n",
        "citation_df_no_blanks = citation_df[citation_df.Parenthetical_Counts > 0]\n",
        "citation_df_no_blanks"
      ],
      "metadata": {
        "id": "ep8NnuJB6JPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "citation_df.to_csv('citation_df_no_blanks.csv') \n",
        "files.download('citation_df_no_blanks.csv')"
      ],
      "metadata": {
        "id": "rPXlUVqBFpuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each paragraph as a txt file\n",
        "#Add each text to a new list called paragraphs\n",
        "citation_paragraphs = []\n",
        "for row in citation_df_no_blanks['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    citation_paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in citation_df_no_blanks['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir citation_paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in citation_paragraphs:\n",
        "  f = open(\"citation_paragraphs/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r citation_paragraphs.zip citation_paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('citation_paragraphs.zip')"
      ],
      "metadata": {
        "id": "u5oQFdSdFvwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this text mining, we have two new data sets to analyze: \n",
        "\n",
        "*   `citation_df.csv`: A CSV file containing each paragraph where rhetorical terminology was used, along with relevant metadata (can be used for close-reading, frequency and regression analysis, PCA)\n",
        "*  `citation_paragraphs.zip`: A zip file containing plain txt versions of each paragraph where rhetorical terminology was used (can be used for close-reading, DocuScope analysis, topic modeling, and/or other types of corpus analysis)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax9DhFPMGvYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analyze Language Pattern Differences Between Scores\n",
        "This section uses frequency plots and regression analysis to determine whether rhetorical analysis term usage and/or citation practice usage are good indicators of score. "
      ],
      "metadata": {
        "id": "WsRg7V5fHIvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rhetorical Terms Regression Analysis"
      ],
      "metadata": {
        "id": "WwmA5Kv1JfXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MbDKMScBxLD"
      },
      "outputs": [],
      "source": [
        "#We need the metadata and text with newlines here; we'll also take the nostops text for further count analysis\n",
        "rhetorical_keywords_df_full_texts = clean_essay_grades_df[['ID', 'Section', 'Portfolio_Score', 'Text_Newlines', 'NoStops_Text']].copy()\n",
        "\n",
        "#Add ID and score in one column\n",
        "rhetorical_keywords_df_full_texts['Score_ID'] = 'Score: ' + rhetorical_keywords_df_full_texts['Portfolio_Score'].astype(str) + ', ID:' + rhetorical_keywords_df_full_texts['ID'].astype(str)\n",
        "\n",
        "#Check new df\n",
        "rhetorical_keywords_df_full_texts.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count usage of each term in each essay\n",
        "pathos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('pathos')\n",
        "ethos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('ethos')\n",
        "logos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('logos')\n",
        "\n",
        "#Append each count to the dataframe\n",
        "rhetorical_keywords_df_full_texts['Pathos_Counts'] = pathos_counts\n",
        "rhetorical_keywords_df_full_texts[\"Ethos_Counts\"] = ethos_counts\n",
        "rhetorical_keywords_df_full_texts[\"Logos_Counts\"] = logos_counts\n",
        "\n",
        "#Get summ of all term usages\n",
        "rhetorical_terms = ['Pathos_Counts', 'Ethos_Counts', 'Logos_Counts']\n",
        "rhetorical_keywords_df_full_texts['Sum_Terms'] = rhetorical_keywords_df_full_texts[rhetorical_terms].sum(axis=1)\n",
        "\n",
        "rhetorical_keywords_df_full_texts"
      ],
      "metadata": {
        "id": "NQlnfgqefBdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times each term was used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Pathos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Pathos_Counts\"]),\n",
        "    go.Bar(name='Ethos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Ethos_Counts\"]),\n",
        "    go.Bar(name='Logos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Logos_Counts\"]),\n",
        "    go.Bar(name='All Term Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Sum_Terms\"]),\n",
        "\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Each Rhetorical Term in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "gaFnyucvshQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Sum_Terms'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Sum Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Sum Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Total Rhetorical Terms is \" + str(r))"
      ],
      "metadata": {
        "id": "NkfyNWfby4XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of usages of pathos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Pathos_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Pathos Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Pathos Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Pathos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of logos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Logos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Logos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of ethos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Ethos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Ethos is \" + str(r))"
      ],
      "metadata": {
        "id": "RnHA4zt-shS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot # paragraphs in which terms were used vs. essay grade\n",
        "##In other words, do more successful writers use terms in multiple paragrpahs (indicating more coherence)?\n",
        "\n",
        "#Count number of paragraphs where terms used and append to new dataframe\n",
        "new_Series = rhetorical_keywords_paragraphs_df_no_blanks['Score_ID'].value_counts(ascending=True)\n",
        "df3 = pd.DataFrame(new_Series).reset_index()\n",
        "df3\n",
        "\n",
        "df3.rename(columns={\"index\": \"Score_ID\", \"Score_ID\": \"Paragraph_Counts\"}, errors=\"raise\", inplace=True)\n",
        "df3[['ID','Score']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "\n",
        "df3\n",
        "\n",
        "#Plot paragraph counts per paper\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Paragraph Counts', x=df3[\"Score_ID\"], y=df3[\"Paragraph_Counts\"]),\n",
        "\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Number of Paragraphs Where Rhetorical Terms Were Used')\n",
        "fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QqTjuT92QSls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3[['Score','ID']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "df3['Score'] = df3['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "df3 = df3[['Score','Paragraph_Counts']].copy()\n",
        "df3 = df3.apply(pd.to_numeric)\n",
        "df3"
      ],
      "metadata": {
        "id": "TLiA3AQcwXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of paragraph term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "#Check if amount of usages of all terms per paragraph is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "\n",
        "\n",
        "x = np.array(df3['Score'])\n",
        "y = np.array(df3['Paragraph_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Paragraph Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Paragrah Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Terms per Paragraph is \" + str(r))"
      ],
      "metadata": {
        "id": "yEE4JNRrQSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do the same thing with the synonyms to see if this makes a difference. "
      ],
      "metadata": {
        "id": "dgMnygollOAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get counts of synonym term usages in full texts\n",
        "full_text_rhetorical_synonym_df = rhetorical_keywords_df_full_texts.copy()\n",
        "full_text_rhetorical_synonym_df['Pathos_Synonyms'] = full_text_rhetorical_synonym_df['NoStops_Text'].str.count('experience|feel|stories|story|understand|compassion|passion|anecdote|sad|anger|sympathy|sympathetic|empathy|pity|fear*')\n",
        "full_text_rhetorical_synonym_df['Logos_Synonyms'] = full_text_rhetorical_synonym_df['NoStops_Text'].str.count('logic|reason|reasoning|statistic|fact|data|common sense|evidence')\n",
        "full_text_rhetorical_synonym_df['Ethos_Synonyms'] = full_text_rhetorical_synonym_df['NoStops_Text'].str.count('credible|credibility|authority|ethic|ethical|reliable')\n",
        "full_text_rhetorical_synonym_df['Rhetorical_Vocab'] = full_text_rhetorical_synonym_df['NoStops_Text'].str.count('audience|reader|context|rhetorical|element|device|appeal|effective')\n",
        "\n",
        "#Get summ of each type of term usages\n",
        "pathos_terms = ['Pathos_Counts', 'Pathos_Synonyms']\n",
        "full_text_rhetorical_synonym_df['Sum_Pathos_Terms'] = full_text_rhetorical_synonym_df[pathos_terms].sum(axis=1)\n",
        "\n",
        "logos_terms = ['Logos_Counts', 'Logos_Synonyms']\n",
        "full_text_rhetorical_synonym_df['Sum_Logos_Terms'] = full_text_rhetorical_synonym_df[logos_terms].sum(axis=1)\n",
        "\n",
        "ethos_terms = ['Ethos_Counts', 'Ethos_Synonyms']\n",
        "full_text_rhetorical_synonym_df['Sum_Ethos_Terms'] = full_text_rhetorical_synonym_df[ethos_terms].sum(axis=1)\n",
        "\n",
        "\n",
        "#Get sum of all term usages\n",
        "all_terms = ['Sum_Pathos_Terms', 'Sum_Ethos_Terms', 'Sum_Ethos_Terms', 'Rhetorical_Vocab']\n",
        "full_text_rhetorical_synonym_df['Sum_All_Terms'] = full_text_rhetorical_synonym_df[all_terms].sum(axis=1)\n",
        "full_text_rhetorical_synonym_df.head()"
      ],
      "metadata": {
        "id": "27XiCwGCmwcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times each type of term was used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Sum_Pathos_Terms', x=rhetorical_synonym_df[\"Score_ID\"], y=rhetorical_synonym_df[\"Sum_Pathos_Terms\"]),\n",
        "    go.Bar(name='Sum_Ethos_Terms', x=rhetorical_synonym_df[\"Score_ID\"], y=rhetorical_synonym_df[\"Sum_Ethos_Terms\"]),\n",
        "    go.Bar(name='Sum_Logos_Terms', x=rhetorical_synonym_df[\"Score_ID\"], y=rhetorical_synonym_df[\"Sum_Logos_Terms\"]),\n",
        "    go.Bar(name='Sum_Rhetorical_Vocab', x=rhetorical_synonym_df[\"Score_ID\"], y=rhetorical_synonym_df[\"Rhetorical_Vocab\"])\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Each Type of Rhetorical Term in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "OzyOOkThlNF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new df for numerical values for regression calculations\n",
        "rhetorical_regression_df = rhetorical_synonym_df[['Score','Sum_Pathos_Terms','Sum_Ethos_Terms',\t'Sum_Logos_Terms','Sum_All_Terms', 'Rhetorical_Vocab']].copy()\n",
        "rhetorical_regression_df = rhetorical_regression_df.apply(pd.to_numeric) \n",
        "rhetorical_regression_df"
      ],
      "metadata": {
        "id": "IIGb8xhflicO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of usages of pathos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Sum_Pathos_Terms'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Pathos Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Pathos Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Pathos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of logos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Sum_Logos_Terms'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Logos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of ethos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Sum_Ethos_Terms'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Ethos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of rhetorical term usages is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Rhetorical_Vocab'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Rhetorical Vocab is \" + str(r))\n",
        "\n"
      ],
      "metadata": {
        "id": "MtfBgjUclW2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of all term usages is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Sum_All_Terms'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for All Terms is \" + str(r))"
      ],
      "metadata": {
        "id": "7lQRN0sVlXCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citation Practice Regression Analysis\n"
      ],
      "metadata": {
        "id": "2WYNa-yxJ0tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using FULL TEXTS Get any text inside parentheticals and count of parentheticals and append to dataframe\n",
        "#https://stackoverflow.com/questions/24696715/regex-for-match-parentheses-in-python\n",
        "parentheticals = r'(?<=\\().*?(?=\\))'\n",
        "\n",
        "parenthetical_matches = []\n",
        "parenthetical_counts = []\n",
        "\n",
        "citation_df_full_texts = clean_essay_grades_df[['ID', 'Section', 'Portfolio_Score','Text']].copy()\n",
        "for text in citation_df_full_texts['Text']:\n",
        "  matches = re.findall(parentheticals, text)\n",
        "  parenthetical_matches.append(matches)\n",
        "  parenthetical_counts.append(len(matches))\n",
        "\n",
        "citation_df_full_texts[\"Parentheticals\"] = parenthetical_matches\n",
        "citation_df_full_texts['Parenthetical_Counts'] = parenthetical_counts\n",
        "citation_df_full_texts"
      ],
      "metadata": {
        "id": "Flph8YEZb-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add ID and score in one column\n",
        "citation_df_full_texts['Score_ID'] = 'Score: ' + citation_df_full_texts['Portfolio_Score'].astype(str) + ', ID:' + citation_df_full_texts['ID'].astype(str)"
      ],
      "metadata": {
        "id": "JQnqKx6BZkql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times parentheticals were used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Parenthetical_Tags', x=citation_df_full_texts[\"Score_ID\"], y=citation_df_full_texts[\"Parenthetical_Counts\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Parentheticals Used in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "KDOgeBIvd4bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regression: Parentheticals vs. Grade\n",
        "\n",
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(citation_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(citation_df_full_texts['Parenthetical_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Parenthetical Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Parenthetical Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Parentheticals is \" + str(r))\n"
      ],
      "metadata": {
        "id": "t17Lgbdcd4dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot # paragraphs in which terms were used vs. essay grade\n",
        "##In other words, do more successful writers use terms in multiple paragrpahs (indicating more coherence)?\n",
        "\n",
        "#Count number of paragraphs where terms used and append to new dataframe\n",
        "new_Series = citation_df['Score_ID'].value_counts(ascending=True)\n",
        "df3 = pd.DataFrame(new_Series).reset_index()\n",
        "df3\n",
        "\n",
        "df3.rename(columns={\"index\": \"Score_ID\", \"Score_ID\": \"Paragraph_Counts\"}, errors=\"raise\", inplace=True)\n",
        "df3[['ID','Score']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "\n",
        "df3\n",
        "\n",
        "#Plot paragraph counts per paper\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Paragraph Counts', x=df3[\"Score_ID\"], y=df3[\"Paragraph_Counts\"]),\n",
        "\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Number of Paragraphs Where Citation Terms Were Used')\n",
        "fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FdDgTs-mKtAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3[['Score','ID']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "df3['Score'] = df3['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "df3 = df3[['Score','Paragraph_Counts']].copy()\n",
        "df3 = df3.apply(pd.to_numeric)\n",
        "df3"
      ],
      "metadata": {
        "id": "krqsS0OfX40t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of paragraph term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "#Check if amount of usages of all terms per paragraph is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "\n",
        "\n",
        "x = np.array(df3['Score'])\n",
        "y = np.array(df3['Paragraph_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Paragraph Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Paragrah Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Parentheticals per Paragraph is \" + str(r))"
      ],
      "metadata": {
        "id": "yENb8z6OKxNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis below uses output from the DocuScope Corpus Analysis platform. This platform is freely available for download from Carnegie Melon University:https://www.cmu.edu/dietrich/english/research-and-publications/docuscope.html\n",
        " \n",
        "\n",
        "DocuScope is a dictionary-based tool that \"tags\" words and phrases in texts based on its 50+ categories of rhetorical primers. The tool might tag the words “according to,” and “is proposing that” as evidence that the student is engaging in citation, for example. In aggregate, these counts can indicate to what degree each text in a corpus contains language indicating a particular rhetorical effect. Our interest in this case is the language DocuScope has tagged as indicating \"Citation\" is occuring; this language can be isolated from the CSV generated from the DocuScope tool, and an example can be found on this Github repository. "
      ],
      "metadata": {
        "id": "KbdHAh7HXZEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Let's do the same using DocuScope citation data\n",
        "#Upload csv with LAT data\n",
        "uploaded_LATS = files.upload()"
      ],
      "metadata": {
        "id": "js39WW-IRH7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lats_df = pd.read_csv('DIMENSION_C_deidentified_texts_citation_clusters_dimensions.csv')\n",
        "lats_df"
      ],
      "metadata": {
        "id": "kiD4Wyu1QYSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make ID document to merge docuscope lats with\n",
        "ids = clean_essay_grades_df[['ID', 'Portfolio_Score']].copy()\n",
        "ids\n",
        "\n",
        "#Rename filename column to id and merge target and LAT tables based on ID\n",
        "lats_df.rename(columns={\"Filename\": \"ID\"}, inplace=True)\n",
        "lats_df['ID'] = lats_df['ID'].map(lambda x: x.rstrip('.txt'))\n",
        "lats_df['ID'] = lats_df['ID'].astype('float')\n",
        "merged_lat_df = pd.merge(ids, lats_df, on='ID')\n",
        "\n",
        "#Add ID and score in one column\n",
        "merged_lat_df['Score_ID'] = 'Score: ' + merged_lat_df['Portfolio_Score'].astype(str) + ', ID:' + merged_lat_df['ID'].astype(str)\n",
        "merged_lat_df"
      ],
      "metadata": {
        "id": "dVL3crKERPur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times all terms were used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Citations_Tags', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"Citation\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of All Citation Cluster Terms Used in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ON5Qx4ZLP5Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regression: Citation vs. Grade\n",
        "\n",
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(merged_lat_df['Portfolio_Score'])\n",
        "y = np.array(merged_lat_df['Citation'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Citation Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Citation Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Total Citation Terms is \" + str(r))\n"
      ],
      "metadata": {
        "id": "Yw8qjZvEPkXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times each citation dimension was used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='CitationAuthority', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationAuthority\"]),\n",
        "    go.Bar(name='CitationControversy', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationControversy\"]),\n",
        "    go.Bar(name='CitationGeneric', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationGeneric\"]),\n",
        "    go.Bar(name='CitationHedged', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationHedged\"]),\n",
        "    go.Bar(name='CitationNegative', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationNegative\"]),\n",
        "    go.Bar(name='CitationNeutral', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationNeutral\"]),\n",
        "    go.Bar(name='CitationSpeakerLookMood', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"CitationSpeakerLookMood\"]),\n",
        "    go.Bar(name='UncertainCitation', x=merged_lat_df[\"Score_ID\"], y=merged_lat_df[\"UncertainCitation\"]),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Each Rhetorical Term in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "_26xY6ykSBa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

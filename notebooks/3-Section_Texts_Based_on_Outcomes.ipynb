{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XRRClG_JJi8f",
        "aU89oBa3JkjG",
        "IefXoHIwNcjo"
      ],
      "authorship_tag": "ABX9TyPUdKD46YxtzHz0TYVUi0/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/notebooks/Section_Texts_Based_on_Outcomes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload Texts for Analysis"
      ],
      "metadata": {
        "id": "Q4bCDOmXrrGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j85gg57FrAlw"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet tsv file to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zvofB_s1rjxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add file into dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['all_output.tsv']), index_col=0, sep=',')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-SasPFdtsSOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Sections of Each Essay Containing Rhetorical Analysis Terms\n",
        "\n",
        "Outcome: *To learn to employ rhetorical terms and strategies and strengthen your ability to analyze rhetorical techniques in published essays and visual texts.*"
      ],
      "metadata": {
        "id": "uZE6NgWarW8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only need one version of the cleaned text for this essay\n",
        "df_rhetorical = df.drop([\"lemma_list\", \"Text_Lowercased\", \"Text_NoHeaders\", 'Text_NoPunct', \"pos_list\", \"ent_list\"], axis=1)\n",
        "df_rhetorical.head()"
      ],
      "metadata": {
        "id": "fQav9GAVs_0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up column for score plus ID\n",
        "df_rhetorical['ID + Score'] = df_rhetorical['ID'].astype(str) + '_' + df_rhetorical['Final Portfolio'].astype(str)\n",
        "\n",
        "#Count number of occurences of rhetorical terms in each paper\n",
        "pathos_counts = df_rhetorical['no_stops'].str.count('pathos')\n",
        "ethos_counts = df_rhetorical['no_stops'].str.count('ethos')\n",
        "logos_counts = df_rhetorical['no_stops'].str.count('logos')\n",
        "\n",
        "df_rhetorical.head()\n"
      ],
      "metadata": {
        "id": "RPXFGApJrj0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph number of pathos, ethos and logos mentions across essays\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Pathos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Pathos_Counts\"]),\n",
        "    go.Bar(name='Ethos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Ethos_Counts\"]),\n",
        "    go.Bar(name='Logos Counts', x=df_rhetorical[\"Final Portfolio\"], y=df_rhetorical[\"Logos_Counts\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AsL96-oJqxB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use concordancing to get context around each rhetorical term\n",
        "\n",
        "def concordance(ci, word, width=400, lines=25):\n",
        "    \"\"\"\n",
        "    Rewrite of nltk.text.ConcordanceIndex.print_concordance that returns results\n",
        "    instead of printing them. \n",
        "\n",
        "    See:\n",
        "    http://www.nltk.org/api/nltk.html#nltk.text.ConcordanceIndex.print_concordance\n",
        "    \"\"\"\n",
        "    half_width = (width - len(word) - 2) // 2\n",
        "    context = width // 2 # approx number of words of context\n",
        "\n",
        "    results = []\n",
        "    offsets = ci.offsets(word)\n",
        "    if offsets:\n",
        "        lines = min(lines, len(offsets))\n",
        "        for i in offsets:\n",
        "            if lines <= 0:\n",
        "                break\n",
        "            left = (' ' * half_width +\n",
        "                    ' '.join(ci._tokens[i-context:i]))\n",
        "            right = ' '.join(ci._tokens[i+1:i+context])\n",
        "            left = left[-half_width:]\n",
        "            right = right[:half_width]\n",
        "            results.append('%s %s %s' % (left, ci._tokens[i], right))\n",
        "            lines -= 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "f5x5LJ6qIjs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out concordancing on one sentence\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import Text, word_tokenize\n",
        "\n",
        "test= \"This is a test. These are test sentences. This is another test sentence. There are many test sentences, but this one is the best.\"\n",
        "from  nltk.text import ConcordanceIndex\n",
        "\n",
        "ci = ConcordanceIndex((word_tokenize(test)))\n",
        "results = concordance(ci, 'is')\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "yZw_j_eyLwDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get context around each instance of pathos in each essay and append to dataframe\n",
        "pathos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'pathos')\n",
        "  pathos_results.append(results)\n",
        "\n",
        "pathos_df = pd.DataFrame(pathos_results)\n",
        "\n",
        "\n",
        "pathos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "pathos_df.head()\n"
      ],
      "metadata": {
        "id": "YuyNWBMBN6OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get similar context around use of pathos in each essay\n",
        "similar_pathos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'pathos')\n",
        "  similar_pathos_results.append(results)\n",
        "\n",
        "similar_pathos_results"
      ],
      "metadata": {
        "id": "LRtfVhBBnddP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Associate each instance with score and reset index, tidy column names\n",
        "pathos_df = pathos_df.set_index('ID_Score')\n",
        "pathos_clean = pathos_df.stack().reset_index()\n",
        "\n",
        "pathos_clean.columns = [\"ID_Score\",\"Pathos_Count\",\"Pathos_Context\"]\n",
        "\n",
        "pathos_clean.head()"
      ],
      "metadata": {
        "id": "7rIXrFpD-LSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the above steps for ethos concordancing\n",
        "\n",
        "#Get context around each instance of ethos in each essay and append to dataframe\n",
        "ethos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'ethos')\n",
        "  ethos_results.append(results)\n",
        "\n",
        "ethos_df = pd.DataFrame(ethos_results)\n",
        "\n",
        "\n",
        "ethos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "#Associate each instance with score and reset index, tidy column names\n",
        "ethos_df = ethos_df.set_index('ID_Score')\n",
        "ethos_clean = ethos_df.stack().reset_index()\n",
        "\n",
        "ethos_clean.columns = [\"ID_Score\",\"Ethos_Count\",\"Ethos_Context\"]\n",
        "\n",
        "ethos_clean.head()\n"
      ],
      "metadata": {
        "id": "C78YMroCAnw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the above steps for logos concordancing\n",
        "#Repeat the above steps for ethos concordancing\n",
        "\n",
        "#Get context around each instance of ethos in each essay and append to dataframe\n",
        "logos_results = []\n",
        "for text in df_rhetorical['no_stops']:\n",
        "  ci = ConcordanceIndex((word_tokenize(text)))\n",
        "  results = concordance(ci, 'logos')\n",
        "  logos_results.append(results)\n",
        "\n",
        "logos_df = pd.DataFrame(logos_results)\n",
        "\n",
        "\n",
        "logos_df.insert(loc = 0,\n",
        "          column = 'ID_Score',\n",
        "          value = df_rhetorical['ID + Score'])\n",
        "\n",
        "#Associate each instance with score and reset index, tidy column names\n",
        "logos_df = logos_df.set_index('ID_Score')\n",
        "logos_clean = logos_df.stack().reset_index()\n",
        "\n",
        "logos_clean.columns = [\"ID_Score\",\"Logos_Count\",\"Logos_Context\"]\n",
        "\n",
        "logos_clean.head()\n"
      ],
      "metadata": {
        "id": "pdj_6bsBApy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine into single dataframe and download\n",
        "import functools as ft\n",
        "rhetorical_dfs = [logos_clean, pathos_clean, ethos_clean]\n",
        "rhetorical_df_final = ft.reduce(lambda left, right: pd.merge(left, right, on='ID_Score'), rhetorical_dfs)\n",
        "\n",
        "#Clean by removing duplicate values of each (replicated during merge)\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Pathos_Context'].duplicated(), 'Pathos_Context'] = 'None'\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Logos_Context'].duplicated(), 'Logos_Context'] = 'None'\n",
        "rhetorical_df_final.loc[rhetorical_df_final['Ethos_Context'].duplicated(), 'Ethos_Context'] = 'None'\n",
        "\n",
        "rhetorical_df_final\n",
        "\n",
        "#Download file to tsv\n",
        "rhetorical_df_final.to_csv('rhetorical_context_df.tsv', encoding = 'utf-8-sig') \n",
        "files.download('rhetorical_context_df.tsv')"
      ],
      "metadata": {
        "id": "aDF9zDulAxDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Sections of Each Essay Containing Citations\n",
        "\n",
        "Outcome: To learn to employ academic evidence "
      ],
      "metadata": {
        "id": "XRRClG_JJi8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://ideone.com/IqZvxm\n",
        "#https://levelup.gitconnected.com/count-citations-in-a-word-document-with-python-and-regular-expressions-d068218c50b9\n",
        "\n",
        "import re\n",
        "pattern = r'\\(([^\"\\)]*|\\bAnonymous\\b|\"[^\"\\)]*\")(, )([\\d]+|n\\.d\\.|[\\d]+[\\w])\\)'\n",
        "num_replaces = 100000000\n",
        "\n",
        "\n",
        "# Try to find citation matches (returned as an iterator of matches)\n",
        "citation_results = []\n",
        "for text in df['Text_NoHeaders']:\n",
        "  results = re.finditer(pattern, text)\n",
        "  citation_results.append(results)\n",
        "\n",
        "citation_results"
      ],
      "metadata": {
        "id": "1nfcw_Jdu5Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Sections of Each Essay Containing Argumentative Terms\n",
        "\n",
        "Outcome: To develop competent academic arguments "
      ],
      "metadata": {
        "id": "aU89oBa3JkjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.sciencedirect.com/science/article/abs/pii/S147515851730005X"
      ],
      "metadata": {
        "id": "GrKD9AU21I5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old code"
      ],
      "metadata": {
        "id": "IefXoHIwNcjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rhetorical_context_df = pd.merge(logos_clean, pathos_clean,how='left', on='ID_Score')\n",
        "rhetorical_context_df = rhetorical_context_df.merge(ethos_clean, how='left', on='ID_Score')\n",
        "\n",
        "#Clean by removing duplicate values of each (replicated during merge)\n",
        "rhetorical_context_df.loc[rhetorical_context_df['Pathos_Context'].duplicated(), 'Pathos_Context'] = 'None'\n",
        "rhetorical_context_df.loc[rhetorical_context_df['Logos_Context'].duplicated(), 'Logos_Context'] = 'None'\n",
        "rhetorical_context_df.loc[rhetorical_context_df['Ethos_Context'].duplicated(), 'Ethos_Context'] = 'None'\n",
        "\n",
        "rhetorical_context_df\n",
        "\n",
        "#Download file to tsv\n",
        "rhetorical_context_df.to_csv('rhetorical_context_df.tsv', encoding = 'utf-8-sig') \n",
        "files.download('rhetorical_context_df.tsv')"
      ],
      "metadata": {
        "id": "InF0XYrzMWGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://avidml.wordpress.com/2017/08/05/natural-language-processing-concordance/\n",
        "def concordanceBySentence(Ngram, rawText):\n",
        "# Input: Takes raw text, tekeonzes it into sentences and tries to find the\n",
        "# NGram that was passed in.\n",
        "#\n",
        "# Output: returns a list of sentences containing that NGram.\n",
        " \n",
        "# This variable holds the list of sentences matching the topNGram, if any.\n",
        " \n",
        "    matchingSentenceList = []\n",
        " \n",
        "    print('\\nWhole sentence concordance for the N-gram: %s' % (Ngram))\n",
        " \n",
        "    sentenceList = nltk.sent_tokenize(rawText)\n",
        " \n",
        "    Ngram = Ngram.lower()\n",
        " \n",
        "    # Get a sentence then see if that Ngram exists in that sentence.\n",
        "    # if it is then append the sentence to the sentence list return variable.\n",
        "    for sentence in sentenceList:\n",
        "        if Ngram in sentence.lower():\n",
        "            matchingSentenceList.append(sentence)\n",
        " \n",
        "    if (len(matchingSentenceList) == 0):\n",
        "        print('No sentences were found with the N-gram: %s \\n' % (Ngram))\n",
        " \n",
        "    return matchingSentenceList\n",
        " "
      ],
      "metadata": {
        "id": "R6-tIFcIKup0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import Text, word_tokenize\n",
        "\n",
        "text = 'This is a sentence'\n",
        "c_text = Text(word_tokenize(text))\n",
        "new_concordance('pathos', c_text, width=200)"
      ],
      "metadata": {
        "id": "tvO2x2UGLIsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with concordancing\n",
        "#https://www.nltk.org/howto/concordance.html\n",
        "import nltk\n",
        "from nltk import Text, word_tokenize\n",
        "\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  c_text = Text(word_tokenize(text))\n",
        "  new_concordance('pathos', c_text, width=200)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "pHI-oSYnJHl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with concordancing\n",
        "#https://www.nltk.org/howto/concordance.html\n",
        "import nltk\n",
        "from nltk import Text, word_tokenize\n",
        "\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  c_text = Text(word_tokenize(text))\n",
        "  con_list = c_text.concordance('pathos')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WueuYWMPBY-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "metadata": {
        "id": "WyJSuP8FIVEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with concordancing\n",
        "#https://www.nltk.org/howto/concordance.html\n",
        "import nltk\n",
        "from nltk import Text, word_tokenize\n",
        "all_con_list = []\n",
        "\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  c_text = Text(word_tokenize(text))\n",
        "  con_list = c_text.concordance_list('pathos')\n",
        "  all_con_list.append(con_list)\n",
        "all_con_list\n"
      ],
      "metadata": {
        "id": "2utIbvq15lu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_con_list[0][0]"
      ],
      "metadata": {
        "id": "rmwiIJ3O5Zb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2lL60L-KFdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/sgsinclair/alta/blob/a482d343142cba12030fea4be8f96fb77579b3ab/ipynb/utilities/Concordances.ipynb\n",
        "def makeConc(word2conc,list2FindIn,context2Use,concList):\n",
        "    # Lets get \n",
        "    end = len(list2FindIn)\n",
        "    for location in range(end):\n",
        "        if list2FindIn[location] == word2conc:\n",
        "            # Here we check whether we are at the very beginning or end\n",
        "            if (location - context2Use) < 0:\n",
        "                beginCon = 0\n",
        "            else:\n",
        "                beginCon = location - context2Use\n",
        "                \n",
        "            if (location + context2Use) > end:\n",
        "                endCon = end\n",
        "            else:\n",
        "                endCon = location + context2Use + 1\n",
        "                \n",
        "            theContext = (list2FindIn[beginCon:endCon])\n",
        "            concordanceLine = ' '.join(theContext)\n",
        "            # print(str(location) + \": \" + concordanceLine)\n",
        "            concList.append(str(location) + \": \" + concordanceLine)\n",
        "\n",
        "theConc = []\n",
        "makeConc(word2find,listOfTokens,int(context),theConc)\n",
        "theConc[-5:]\n"
      ],
      "metadata": {
        "id": "YUgUOaW2KFiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCxTZJ5RKFlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDU2fg56KFnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6S_KK8cCKFp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://simply-python.com/2014/03/14/saving-output-of-nltk-text-concordance/\n",
        "def get_all_phrases_containing_tar_wrd(target_word, tar_passage, left_margin = 10, right_margin = 10):\n",
        "    \"\"\"\n",
        "        Function to get all the phases that contain the target word in a text/passage tar_passage.\n",
        "        Workaround to save the output given by nltk Concordance function\n",
        "         \n",
        "        str target_word, str tar_passage int left_margin int right_margin --> list of str\n",
        "        left_margin and right_margin allocate the number of words/pununciation before and after target word\n",
        "        Left margin will take note of the beginning of the text\n",
        "    \"\"\"\n",
        "     \n",
        "    ## Create list of tokens using nltk function\n",
        "    tokens = nltk.word_tokenize(tar_passage)\n",
        "     \n",
        "    ## Create the text of tokens\n",
        "    text = nltk.Text(tokens)\n",
        " \n",
        "    ## Collect all the index or offset position of the target word\n",
        "    c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
        " \n",
        "    ## Collect the range of the words that is within the target word by using text.tokens[start;end].\n",
        "    ## The map function is use so that when the offset position - the target range < 0, it will be default to zero\n",
        "    concordance_txt = ([text.tokens[map(lambda x: x-5 if (x-left_margin)>0 else 0,[offset])[0]:offset+right_margin]\n",
        "                        for offset in c.offsets(target_word)])\n",
        "                         \n",
        "    ## join the sentences for each of the target phrase and return it\n",
        "    return [''.join([x+' ' for x in con_sub]) for con_sub in concordance_txt]\n",
        "\n"
      ],
      "metadata": {
        "id": "1C4o3wKaATiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4.dispersion_plot"
      ],
      "metadata": {
        "id": "HHgIvMI92Rf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with keyword function\n",
        "\n",
        "test = []\n",
        "def process(words, search):\n",
        "    #print(words)\n",
        "    if search in words:\n",
        "        pos = words.index(search)\n",
        "        context = words[pos-20:pos+20]\n",
        "        test.append(context)\n",
        "    else:\n",
        "        #return words\n",
        "        return []\n",
        "\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  process(text, \"pathos\")\n"
      ],
      "metadata": {
        "id": "2bUKK1772vUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[1]"
      ],
      "metadata": {
        "id": "Up3HnZ0g36-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with concordancing\n",
        "#https://www.nltk.org/howto/concordance.html\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  results = get_all_phrases_containing_tar_wrd('pathos', text)\n",
        "  for result in results:\n",
        "    print(result)\n",
        " "
      ],
      "metadata": {
        "id": "8naW8lS7Eip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiments with KWIC analysis (ngrams)\n",
        "#https://programminghistorian.org/en/lessons/keywords-in-context-using-n-grams\n",
        "\n",
        "i = 0\n",
        "\n",
        "for text in df_rhetorical['Text_NoHeaders']:\n",
        "  wordlist = text.split()\n",
        "  print(wordlist[0:4])\n",
        "  \n",
        "\n",
        "def getNGrams(wordlist, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(wordlist)-(n-1)):\n",
        "        ngrams.append(wordlist[i:i+n])\n",
        "    return ngrams\n",
        "\n",
        "import obo\n"
      ],
      "metadata": {
        "id": "GJr7_uMly9nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(lambda row: Text(row['Text_NoHeaders']), axis=1)\n",
        "\n",
        "df_rhetorical['Text'] = df_rhetorical.apply(lambda row: Text(row['Text_NoHeaders']), axis=1)\n"
      ],
      "metadata": {
        "id": "UAZL1qoztsGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df_rhetorical['Tokens'] = df_rhetorical.apply(lambda row: nltk.word_tokenize(row['Text_NoHeaders']), axis=1)\n",
        "df_rhetorical\n"
      ],
      "metadata": {
        "id": "PWttM0WM0lia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(words, search):\n",
        "    #print(words)\n",
        "    for word in words:\n",
        "     if search in words:\n",
        "        pos = words.index(search)\n",
        "        return words[pos-10:pos+10]\n",
        "    else:\n",
        "        #return words\n",
        "        return []\n",
        "\n",
        "for \n",
        "\n",
        "\n",
        "df_rhetorical[\"Pathos\"] = df_rhetorical[\"Tokens\"].apply(lambda words:process(words, 'pathos'))\n",
        "\n",
        "df_rhetorical"
      ],
      "metadata": {
        "id": "1CYrNmAt0whC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rhetorical[\"Pathos\"] = df_rhetorical[\"Tokens\"].apply(lambda words:process(words, 'pathos'))\n",
        "\n",
        "df_rhetorical[\"Pathos\"]"
      ],
      "metadata": {
        "id": "h8ThjYFK3Qyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rhetorical.Text_NoHeaders.str.split('pathos', expand=True)\\\n",
        "  .apply(lambda x: x.str.extract('(\\w+)', expand=False))\\\n",
        "  .apply(lambda x: 'pathos '.join(x), 1)"
      ],
      "metadata": {
        "id": "yP4-fUTbvOmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rhetorical.apply(getNGrams(df_rhetorical['Text_NoHeaders'],5))"
      ],
      "metadata": {
        "id": "DOJbaOziwwI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(text,n):\n",
        "    '''Searches for text, and retrieves n words either side of the text, which are retuned seperatly'''\n",
        "    word = r\"\\W*([\\w]+)\"\n",
        "    groups = re.search(r'{}\\W*{}{}'.format(word*n,'place',word*n), text).groups()\n",
        "    return groups[:n],groups[n:]"
      ],
      "metadata": {
        "id": "Z8YvayUqrj3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some basic analyses\n",
        "\n",
        "#How many instances of each term papers scoring in different grade ranges? \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_rhetorical = df_rhetorical.sort_values(by=['Final Portfolio'], ascending=True)\n",
        "\n",
        "df_rhetorical.plot(kind='bar',x='Final Portfolio',y='Pathos_Counts')"
      ],
      "metadata": {
        "id": "m8c7JQxqonJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/notebooks/Preprocessing_and_Basic_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVeD4Ik7D43F"
      },
      "source": [
        "##Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Installs libraries and packages to stem and lemmatize texts\n",
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#Installs NLTK libraries and packages to perform chunking, parsing and visualization\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "!pip install svgling\n",
        "\n",
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ8ve667EvxG"
      },
      "source": [
        "## Load TSV into DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "outputs": [],
      "source": [
        "#Selet tsv file to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "outputs": [],
      "source": [
        "#Add file into dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['test_submissions.tsv']), index_col=0, sep=',')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il5slp5CMKeb"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "outputs": [],
      "source": [
        "#Lowercase all words\n",
        "df['Text'] = df['Text_NoHeaders'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "df['Text'] = df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "df['Text'] = df['Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "df.head()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8xzj76CVRhY"
      },
      "outputs": [],
      "source": [
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df['no_stops'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analysis and Enrichment"
      ],
      "metadata": {
        "id": "WjvWl4YZcuEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word count of each text\n",
        "df['Length'] = df['Text_NoHeaders'].apply(lambda x: len(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "g80qlZ6LZDqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph portfolio grade by length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df.sort_values(by=['Portfolio Score'], ascending=True)\n",
        "\n",
        "df.plot(kind='bar',x='Portfolio Score',y='Length')"
      ],
      "metadata": {
        "id": "zRaiUcS3ZNI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "outputs": [],
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['lemma_list'] = lemma_list\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['pos_list'] = pos_list\n",
        "\n",
        "#Check pos tags\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hyxBSpF0X9ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHZFtLvJCcU-"
      },
      "outputs": [],
      "source": [
        "#Get dependency parsing for single doc and visualize\n",
        "doc = nlp(df.Text_NoHeaders[0]) \n",
        "print(doc)\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "outputs": [],
      "source": [
        "#Get named entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "df['ent_list'] = ent_list\n",
        "\n",
        "#Check named entities\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50yGYViSLo9T"
      },
      "outputs": [],
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(df.Text[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNfVGTLhMdhIpAaV5wZJ7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
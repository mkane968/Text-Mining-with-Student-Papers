{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFxC9VCmzFslUNdvBi3Zx0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/Text_Mining_Student_Essays_A_Computational_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Mining Student Essays: A Computational Exploration\n",
        "\n",
        "This pipeline will ingest, clean and analyze meaningful language patterns in a corpora of student papers. The following input is required: \n",
        "\n",
        "*   Corpus of student papers (.txt files)\n",
        "*   Grades and other relevant metadata associated with the papers (.csv files)\n"
      ],
      "metadata": {
        "id": "MOQkVubJcBq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages"
      ],
      "metadata": {
        "id": "zDd6JVT4cxbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIv9gPl0cBQo"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "#Install glob\n",
        "import glob \n",
        "\n",
        "#Install pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Install numpy\n",
        "import numpy as np\n",
        "\n",
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from  nltk.text import ConcordanceIndex\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Import matplotlib for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import logging  # Setting up the loggings to monitor gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Student Essays and Metadata"
      ],
      "metadata": {
        "id": "61xG-OZIdC0u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_EUmsB-u8i"
      },
      "source": [
        "###Import Student Essays and Add to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqzr4eGV2PYi"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OshZ_q2J-X85"
      },
      "outputs": [],
      "source": [
        "#Add files to upload from local machine\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aaq5zqob-ci5"
      },
      "outputs": [],
      "source": [
        "#Put essays into dataframe\n",
        "essays = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "\n",
        "#Reset index and add column names to make wrangling easier\n",
        "essays = essays.reset_index()\n",
        "essays.columns = [\"ID\", \"Text\"]\n",
        "\n",
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "essays['Text'] = essays['Text'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "#Remove newline characters and put in new column \n",
        "essays['Text_Newlines'] = essays['Text']\n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "essays.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove identifying information from each paper ID (instructor/student names) "
      ],
      "metadata": {
        "id": "Jm37ef7fdwUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNWKKEGI_IV2"
      },
      "outputs": [],
      "source": [
        "#Remove identifying information from ID\n",
        "#Remove any occurences of \"LATE_\" from dataset (otherwise will skew ID cleaning)\n",
        "essays['ID'] = essays['ID'].str.replace(r'LATE_', '', regex=True) \n",
        "\n",
        "#Split book on first underscore (_) in ID, keep only text in between first and second underscore (ID number)\n",
        "start = essays[\"ID\"].str.split(\"_\", expand = True)\n",
        "essays['ID'] = start[1]\n",
        "essays['ID'] = essays['ID'].astype(int)\n",
        "essays"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(essays)"
      ],
      "metadata": {
        "id": "otkAjDcw19n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I22itikk_QwV"
      },
      "source": [
        "### Import grades and additional metadata to second dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzgvrSb6_gnN"
      },
      "outputs": [],
      "source": [
        "#Upload csvs with essay metadata\n",
        "uploaded_grades = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntoCPrN2_xAt"
      },
      "outputs": [],
      "source": [
        "#Link to path where csv files are stored in drive\n",
        "local_path = r'/content'\n",
        "\n",
        "#Create variable to store all csvs in path\n",
        "filenames = glob.glob(local_path + \"/*.csv\")\n",
        "\n",
        "#Create df list for all csvs\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]\n",
        "\n",
        "len(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(filenames)"
      ],
      "metadata": {
        "id": "YPMJzS-_1Otl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all data into one DataFrame\n",
        "metadata = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "#Change data to string (for further cleaning)\n",
        "metadata.astype(str)\n",
        "\n",
        "metadata"
      ],
      "metadata": {
        "id": "uBIcqCFk2PQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgo-19g4_6AN"
      },
      "outputs": [],
      "source": [
        "#Drop header rows(Points Possible) and test student rows (Student, Test)\n",
        "metadata = metadata[metadata['Student'].str.contains('Points Possible|Student, Test')==False]\n",
        "metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep only relevant metadata (ID, Section, Final Portfolio Scores)\n",
        "clean_metadata = metadata[['ID'] + ['Section'] + list(metadata.loc[:, metadata.columns.str.startswith('Final Portfolio (')])]\n",
        "clean_metadata\n",
        "#Want other metadata? Check the columns\n",
        "#Get all column names \n",
        "#for col in metadata.columns:\n",
        "   # print(col)"
      ],
      "metadata": {
        "id": "Et65_490s5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "180g-ohp_8Tq"
      },
      "outputs": [],
      "source": [
        "#Replace all NaN values with 0 \n",
        "clean_metadata = clean_metadata.replace(np.nan, 0)\n",
        "clean_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzvBqD2__TA"
      },
      "outputs": [],
      "source": [
        "#Create new final portfolio column with all values\n",
        "#Add values of each column together; values except correct grade will be zero\n",
        "score_counts = clean_metadata.columns[2:]\n",
        "clean_metadata['Portfolio_Score'] = clean_metadata[score_counts].sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_metadata['Portfolio_Score']"
      ],
      "metadata": {
        "id": "9Fhlww2gokZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGJnhNdZAB9-"
      },
      "outputs": [],
      "source": [
        "#Drop grade columns for individual classes\n",
        "clean_metadata = clean_metadata[['ID', 'Section', \"Portfolio_Score\"]]\n",
        "clean_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz_wEeL0ACdW"
      },
      "outputs": [],
      "source": [
        "#Drop decimal from ID (inconsistent with ID in essay dataframe)\n",
        "clean_metadata['ID'] = clean_metadata['ID'].astype(int)\n",
        "\n",
        "#Check cleaned DF one more time\n",
        "clean_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMhaQbM2AEMC"
      },
      "source": [
        "### Merge essays and grade metadata into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwAjifpvALsP"
      },
      "outputs": [],
      "source": [
        "#Merge metadata and cleaned essays into new dataframe\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "essays_grades_master = clean_metadata.merge(essays,on='ID')\n",
        "\n",
        "#Print dataframe\n",
        "essays_grades_master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort dataframe by grades\n",
        "essays_grades_master.sort_values(by=['Portfolio_Score'], inplace = True)\n",
        "essays_grades_master"
      ],
      "metadata": {
        "id": "eLM2Jz5Gd05R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYmo80TOAPHf"
      },
      "outputs": [],
      "source": [
        "#Save new df to csv and download\n",
        "essays_grades_master.to_csv('essays_grades_master.csv') \n",
        "files.download('essays_grades_master.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clean Data"
      ],
      "metadata": {
        "id": "qIYZO-LW2sGO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZQOgu0Bi15"
      },
      "source": [
        "### Basic Cleaning with NLTK\n",
        "####Lowercasing, Punctuation Removal, and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Ux-p8QAvrZ"
      },
      "outputs": [],
      "source": [
        "#Rename dataframe\n",
        "clean_essay_grades_df = essays_grades_master\n",
        "clean_essay_grades_df.rename(columns = {\"Text_NoHeaders\": \"Text\"}, inplace = True)\n",
        "\n",
        "#Lowercase all words\n",
        "clean_essay_grades_df['Lower_Text'] = clean_essay_grades_df['Text'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['Lower_Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['NoPunct_Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "clean_essay_grades_df['NoStops_Text'] = clean_essay_grades_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Check output\n",
        "clean_essay_grades_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8uBilltBxpC"
      },
      "source": [
        "## 4. Extract Keywords and Context: Rhetorical Analysis\n",
        "\n",
        "**Key Terms:** Pathos, Ethos, Logos\n",
        "\n",
        "**Related Outcome:** *To learn to employ rhetorical terms and strategies and strengthen your ability to analyze rhetorical techniques in published essays and visual texts.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MbDKMScBxLD"
      },
      "outputs": [],
      "source": [
        "#We need the metadata and text with newlines here; we'll also take the nostops text for further count analysis\n",
        "rhetorical_keywords_df = clean_essay_grades_df[['ID', 'Section', 'Portfolio_Score', 'Text_Newlines']].copy()\n",
        "\n",
        "#Add ID and score in one column\n",
        "rhetorical_keywords_df['Score_ID'] = 'Score: ' + rhetorical_keywords_df['Portfolio_Score'].astype(str) + ', ID:' + rhetorical_keywords_df['ID'].astype(str)\n",
        "\n",
        "#Check new df\n",
        "rhetorical_keywords_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paragraph Segmentation"
      ],
      "metadata": {
        "id": "R94UepApDma8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only need one newlines version here\n",
        "rhetorical_keywords_df = rhetorical_keywords_df[['Score_ID', 'Text_Newlines']].copy()\n",
        "\n",
        "#Check new df\n",
        "rhetorical_keywords_df.head()\n"
      ],
      "metadata": {
        "id": "PjxeIWG2HsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of paragraphs in each text\n",
        "paragraph_counts = rhetorical_keywords_df['Text_Newlines'].str.count(r'\\n')\n",
        "paragraph_counts\n",
        "\n",
        "#Append paragraphs counts to dataframe\n",
        "rhetorical_keywords_df[\"Paragraph_Counts\"] = paragraph_counts\n",
        "rhetorical_keywords_df"
      ],
      "metadata": {
        "id": "AKA5Xxo7Dovo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new paragraph starts \n",
        "new = rhetorical_keywords_df[\"Text_Newlines\"].str.split(r'\\n', expand = True).set_index(rhetorical_keywords_df['Score_ID'])\n",
        "\n",
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "paragraphs_df = new.stack().reset_index()\n",
        "paragraphs_df.columns = [\"Score_ID\", \"Paragraph\", \"Text\"]\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "m-B7rbmwGv34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Clean paragraphs\n",
        "##Filter out paragraphs with 5 or less words (headers)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.split().str.len().lt(10)]\n",
        "\n",
        "## Filter out paragraphs containing \"http://\", \"doi:\" , \"https://\" and \"://www\" (Works Cited citations)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"http://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"https://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"://www\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"www.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\".com/\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"Vol.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"doi:\")]"
      ],
      "metadata": {
        "id": "rTz6Y4cyGv6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs_df"
      ],
      "metadata": {
        "id": "9tQ-NMa13TS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download to clean further\n",
        "paragraphs_df.to_csv('paragraphs.csv') \n",
        "files.download('paragraphs.csv')"
      ],
      "metadata": {
        "id": "1RuBKJo9Gv8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Set up new dataframe for keyword frequency counts\n",
        "rhetorical_keywords_df = paragraphs_df.copy()\n",
        "\n",
        "#Count number of occurences of rhetorical terms in each paper\n",
        "pathos_counts = rhetorical_keywords_df['Text'].str.count('pathos')\n",
        "ethos_counts = rhetorical_keywords_df['Text'].str.count('ethos')\n",
        "logos_counts = rhetorical_keywords_df['Text'].str.count('logos')\n",
        "\n",
        "#Append each count to the dataframe\n",
        "rhetorical_keywords_df['Pathos_Counts'] = pathos_counts\n",
        "rhetorical_keywords_df[\"Ethos_Counts\"] = ethos_counts\n",
        "rhetorical_keywords_df[\"Logos_Counts\"] = logos_counts\n",
        "\n",
        "#Get summ of all term usages\n",
        "rhetorical_terms = ['Pathos_Counts', 'Ethos_Counts', 'Logos_Counts']\n",
        "rhetorical_keywords_df['Sum_Terms'] = rhetorical_keywords_df[rhetorical_terms].sum(axis=1)\n",
        "\n",
        "rhetorical_keywords_df"
      ],
      "metadata": {
        "id": "4V7oWhKpSNQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all rows with no rhetorical terms\n",
        "rhetorical_keywords_df_no_blanks = rhetorical_keywords_df[rhetorical_keywords_df.Sum_Terms > 0]\n",
        "rhetorical_keywords_df_no_blanks"
      ],
      "metadata": {
        "id": "lXOWxEyISjkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "rhetorical_keywords_df.to_csv('rhetorical_keywords_df.csv') \n",
        "files.download('rhetorical_keywords_df.csv')"
      ],
      "metadata": {
        "id": "RLCLHX0wWO-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Analyzing Term Count Frequencies\n"
      ],
      "metadata": {
        "id": "m8xHlct1Wf1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times all terms were used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='All Terms', x=rhetorical_keywords_df[\"Score_ID\"], y=rhetorical_keywords_df[\"Sum_Terms\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of All Rhetorical Terms in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "cZv_B9xVTulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get scores on their own to calculate regression\n",
        "rhetorical_keywords_df[['Score','ID']] = rhetorical_keywords_df.Score_ID.str.split(\", \",expand=True)\n",
        "rhetorical_keywords_df['Score'] = rhetorical_keywords_df['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "\n",
        "#Create new df for numerical values for regression calculations\n",
        "rhetorical_regression_df = rhetorical_keywords_df[['Score','Pathos_Counts',\t'Ethos_Counts',\t'Logos_Counts','Sum_Terms']].copy()\n",
        "rhetorical_regression_df = rhetorical_regression_df.apply(pd.to_numeric) \n",
        "rhetorical_regression_df"
      ],
      "metadata": {
        "id": "KXwSkz7Kwk6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Sum_Terms'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Sum Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Sum Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Total Rhetorical Terms is \" + str(r))"
      ],
      "metadata": {
        "id": "IFGP9bN3ZKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times each term was used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Pathos Counts', x=rhetorical_keywords_df[\"Score_ID\"], y=rhetorical_keywords_df[\"Pathos_Counts\"]),\n",
        "    go.Bar(name='Ethos Counts', x=rhetorical_keywords_df[\"Score_ID\"], y=rhetorical_keywords_df[\"Ethos_Counts\"]),\n",
        "    go.Bar(name='Logos Counts', x=rhetorical_keywords_df[\"Score_ID\"], y=rhetorical_keywords_df[\"Logos_Counts\"]),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Each Rhetorical Term in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "n0xbNNrqQSoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of usages of pathos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Pathos_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Pathos Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Pathos Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Pathos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of logos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Logos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Logos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of ethos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_regression_df['Score'])\n",
        "y = np.array(rhetorical_regression_df['Ethos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Ethos is \" + str(r))"
      ],
      "metadata": {
        "id": "cmFxNVRxyg36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot # paragraphs in which terms were used vs. essay grade?\n",
        "##In other words, do more successful writers use terms in multiple paragrpahs (indicating more coherence)?\n",
        "\n",
        "#Count number of paragraphs where terms used and append to new dataframe\n",
        "new_Series = rhetorical_keywords_df_no_blanks['Score_ID'].value_counts(ascending=True)\n",
        "df3 = pd.DataFrame(new_Series).reset_index()\n",
        "df3\n",
        "\n",
        "df3.rename(columns={\"index\": \"Score_ID\", \"Score_ID\": \"Paragraph_Counts\"}, errors=\"raise\", inplace=True)\n",
        "df3[['ID','Score']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "\n",
        "df3\n",
        "\n",
        "#Plot paragraph counts per paper\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Paragraph Counts', x=df3[\"Score_ID\"], y=df3[\"Paragraph_Counts\"]),\n",
        "\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Number of Paragraphs Where Rhetorical Terms Were Used')\n",
        "fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QqTjuT92QSls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3[['Score','ID']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "df3['Score'] = df3['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "df3 = df3[['Score','Paragraph_Counts']].copy()\n",
        "df3 = df3.apply(pd.to_numeric)\n",
        "df3"
      ],
      "metadata": {
        "id": "TLiA3AQcwXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of paragraph term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "#Check if amount of usages of all terms per paragraph is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "\n",
        "\n",
        "x = np.array(df3['Score'])\n",
        "y = np.array(df3['Paragraph_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Paragraph Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Paragrah Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Terms per Paragraph is \" + str(r))"
      ],
      "metadata": {
        "id": "yEE4JNRrQSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rhetorical_keywords_df_no_blanks"
      ],
      "metadata": {
        "id": "nN9oTJD6eHev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rhetorical_df_filtered = rhetorical_keywords_df_no_blanks[rhetorical_keywords_df_no_blanks['Paragraph'] < 50]\n",
        "\n",
        "import plotly.express as px\n",
        "fig = px.line(rhetorical_df_filtered, x='Paragraph', y='Sum_Terms', color='Score_ID', markers=True)\n",
        "fig.update_layout(title_text='Term Usage by Paragraph (B-, C-Range, and D-Range Papers)')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "myTYAH8Acqsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Get Positions of Term Usages"
      ],
      "metadata": {
        "id": "t2xnYKmq1O4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rhetorical_keywords_df_no_blanks"
      ],
      "metadata": {
        "id": "3ulsizRG1S1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n",
        "#Get position of all occurences of pathos\n",
        "pathos_results = []\n",
        "for text in rhetorical_keywords_df_no_blanks.Text:\n",
        "  result = [m.start() for m in re.finditer('pathos', text)]\n",
        "  pathos_results.append(result)\n",
        "\n",
        "rhetorical_keywords_df_no_blanks['Pathos_Positions'] = pathos_results\n",
        "\n",
        "#Get position of all occurences of ethos\n",
        "ethos_results = []\n",
        "for text in rhetorical_keywords_df_no_blanks.Text:\n",
        "  result = [m.start() for m in re.finditer('ethos', text)]\n",
        "  ethos_results.append(result)\n",
        "\n",
        "rhetorical_keywords_df_no_blanks['Ethos_Positions'] = ethos_results\n",
        "\n",
        "#Get position of all occurences of logos\n",
        "logos_results = []\n",
        "for text in rhetorical_keywords_df_no_blanks.Text:\n",
        "  result = [m.start() for m in re.finditer('logos', text)]\n",
        "  logos_results.append(result)\n",
        "\n",
        "rhetorical_keywords_df_no_blanks['Logos_Positions'] = logos_results\n",
        "rhetorical_keywords_df_no_blanks\n"
      ],
      "metadata": {
        "id": "BH3RPRf4PdwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Get Rhetorical Term Synonyms with Word2Vec"
      ],
      "metadata": {
        "id": "ipe5cx0X1LWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe for word2vec\n",
        "word2vec_essays = essays_grades_master[['ID', 'Portfolio_Score', 'NoStops_Text']].copy()\n",
        "word2vec_essays\n",
        "\n",
        "#Split dataframe into three groups based on grades\n",
        "low = 83\n",
        "high = 93\n",
        "c_range = word2vec_essays[word2vec_essays['Portfolio_Score'] <= low]\n",
        "b_range = word2vec_essays[word2vec_essays['Portfolio_Score'] > low]\n",
        "b_range = b_range[word2vec_essays['Portfolio_Score'] <= high]\n",
        "a_range = word2vec_essays[word2vec_essays['Portfolio_Score'] > high]"
      ],
      "metadata": {
        "id": "7oEPwo-QW3sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec on A Range Essays"
      ],
      "metadata": {
        "id": "mZSOXpWQaPgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define spaCy function to lemmatize, remove stopwords and non-alphanumeric characters\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)"
      ],
      "metadata": {
        "id": "9n0z0vgwaPgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove characters\n",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in a_range['NoStops_Text'])\n",
        "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
        "\n",
        "#Put results in a new dataframe\n",
        "df_clean = pd.DataFrame({'clean': txt})\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "df_clean.shape"
      ],
      "metadata": {
        "id": "M1CyrkJIaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use bigrams to detect common phrases\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "#Take list of list of words as input\n",
        "sent = [row.split() for row in df_clean['clean']]\n",
        "\n",
        "#Creates relevant list of phrases from all sentences\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "#Transform the corpus based on the bigrams detected:\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "rQX2I-c1aPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count word frequency\n",
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ],
      "metadata": {
        "id": "aqnYX2hPaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get most frequent words\n",
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ],
      "metadata": {
        "id": "sgNgBrpBaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import word2vec\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
      ],
      "metadata": {
        "id": "sp_8IdjhaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build word2vecmodel (check how to set parameters in tutorial)\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=cores-1)"
      ],
      "metadata": {
        "id": "J2pKSKotaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vocab table--digest all words, filter out unique words, do counts on them\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "PrZxbYoJaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set parameters to train the model\n",
        "t = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "ZpcWVTStaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that the model has been trained, make it more memory efficient\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "metadata": {
        "id": "eHnCgQoBaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to pathos in a-range essays\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\"])"
      ],
      "metadata": {
        "id": "PMFgqAS3aPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "w2v_model.wv.most_similar(positive=[\"ethos\"])"
      ],
      "metadata": {
        "id": "s_57Iml8aPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "#I think I cleaned logos out of the corpus accidentally! Only \"logo\" shows up\n",
        "w2v_model.wv.most_similar(positive=[\"logo\"])"
      ],
      "metadata": {
        "id": "eFdwBi_maPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check similarity between words\n",
        "w2v_model.wv.similarity(\"pathos\", 'logo')"
      ],
      "metadata": {
        "id": "RcbrA6EYaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analogy difference\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\", \"logo\"], negative=[\"emotion\"], topn=3)"
      ],
      "metadata": {
        "id": "mmhZE09KaPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec on B Range Essays"
      ],
      "metadata": {
        "id": "EO_CEjdYayIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define spaCy function to lemmatize, remove stopwords and non-alphanumeric characters\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)"
      ],
      "metadata": {
        "id": "4OvkE15AayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove characters\n",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in b_range['NoStops_Text'])\n",
        "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
        "\n",
        "#Put results in a new dataframe\n",
        "df_clean = pd.DataFrame({'clean': txt})\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "df_clean.shape"
      ],
      "metadata": {
        "id": "mHrcl6kYayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use bigrams to detect common phrases\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "#Take list of list of words as input\n",
        "sent = [row.split() for row in df_clean['clean']]\n",
        "\n",
        "#Creates relevant list of phrases from all sentences\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "#Transform the corpus based on the bigrams detected:\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "ACRie4Y5ayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count word frequency\n",
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ],
      "metadata": {
        "id": "DitG1oIhayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get most frequent words\n",
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ],
      "metadata": {
        "id": "RtjDsu5tayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import word2vec\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
      ],
      "metadata": {
        "id": "hCWLnpWBayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build word2vecmodel (check how to set parameters in tutorial)\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=cores-1)"
      ],
      "metadata": {
        "id": "TbLy9zNZayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vocab table--digest all words, filter out unique words, do counts on them\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "o6O6u7ORayIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set parameters to train the model\n",
        "t = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "4mepZoAzayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that the model has been trained, make it more memory efficient\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "metadata": {
        "id": "KvMey6ERayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\"])"
      ],
      "metadata": {
        "id": "G6bc1Kc_ayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "w2v_model.wv.most_similar(positive=[\"ethos\"])"
      ],
      "metadata": {
        "id": "Q3dqW1B3ayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "#I think I cleaned logos out of the corpus accidentally! Only \"logo\" shows up\n",
        "w2v_model.wv.most_similar(positive=[\"logo\"])"
      ],
      "metadata": {
        "id": "zUzLWQOSayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check similarity between words\n",
        "w2v_model.wv.similarity(\"pathos\", 'ethos')"
      ],
      "metadata": {
        "id": "UsXXgRQ5ayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analogy difference\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\", \"ethos\"], negative=[\"emotion\"], topn=3)"
      ],
      "metadata": {
        "id": "JJaUS594ayIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec on C Range Essays"
      ],
      "metadata": {
        "id": "M9RVGCvlZqF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define spaCy function to lemmatize, remove stopwords and non-alphanumeric characters\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)"
      ],
      "metadata": {
        "id": "-TBJ_VbyWQ1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove characters\n",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in c_range['NoStops_Text'])\n",
        "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
        "\n",
        "#Put results in a new dataframe\n",
        "df_clean = pd.DataFrame({'clean': txt})\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "df_clean.shape"
      ],
      "metadata": {
        "id": "_lUe-PMVWQ4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use bigrams to detect common phrases\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "#Take list of list of words as input\n",
        "sent = [row.split() for row in df_clean['clean']]\n",
        "\n",
        "#Creates relevant list of phrases from all sentences\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "#Transform the corpus based on the bigrams detected:\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "b0WKv5K0WQ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count word frequency\n",
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ],
      "metadata": {
        "id": "8JDAdfqvWvh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get most frequent words\n",
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ],
      "metadata": {
        "id": "nJ5ZnA2vWw3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import word2vec\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
      ],
      "metadata": {
        "id": "gEfTtUnZWyV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build word2vecmodel (check how to set parameters in tutorial)\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=cores-1)"
      ],
      "metadata": {
        "id": "HRUnfN3cZ11L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vocab table--digest all words, filter out unique words, do counts on them\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "2rkbJdIaZ33I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set parameters to train the model\n",
        "t = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "bqc2SanwZ6Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that the model has been trained, make it more memory efficient\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "metadata": {
        "id": "2-IQ6I5cZ8n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus of c-range essays\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\"])"
      ],
      "metadata": {
        "id": "KaZvE5eyZ-Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "w2v_model.wv.most_similar(positive=[\"ethos\"])"
      ],
      "metadata": {
        "id": "sciMMH_saA5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find most similar words to key terms in corpus\n",
        "#I think I cleaned logos out of the corpus accidentally! Only \"logo\" shows up\n",
        "w2v_model.wv.most_similar(positive=[\"logo\"])"
      ],
      "metadata": {
        "id": "TSNygJwzaC_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check similarity between words\n",
        "w2v_model.wv.similarity(\"pathos\", 'ethos')"
      ],
      "metadata": {
        "id": "12HpAyHGaG7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analogy difference\n",
        "w2v_model.wv.most_similar(positive=[\"pathos\", \"ethos\"], negative=[\"emotion\"], topn=3)"
      ],
      "metadata": {
        "id": "nsSXz2RZaJZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

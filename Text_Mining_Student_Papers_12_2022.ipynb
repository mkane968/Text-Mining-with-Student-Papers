{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Y5URtri2mnwLZXgfcpNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-with-Student-Papers/blob/main/Text_Mining_Student_Papers_12_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Mining Student Papers\n",
        "## Introduction\n",
        "How should students' learning be measured in first-year writing courses? SOME LINK. In the Fall 2022 Cultural Analytics practicum, I investigated the value of text mining to first-year writing program assessment. This project was developed in the context of Temple University's First-Year Writing Program, with attention to the specific outcomes they expect students to attain upon successfully completing first-year writing courses. Given the current constraints of my corpus (a pilot sample, not wholly representative of the data I plan to collect for my full dissertation study), I focused on two broad outcomes:\n",
        "\n",
        "1.   Use rhetorical terminology and principles to analyze how a source makes an argument\n",
        "2.   Identify and engage with arguments made in secondary sources\n",
        "\n",
        "The Python code I've developed over the course of this semester demonstrates two main ways that text-mining can support program assessment. First, provides a reliable and efficient method of identifying key passages in a corpora of student work where each outcome is operationalized, based on terminology and practices valuable to the researcher/instructor. Further, text-mining illuminates distinct ways students use language when operationalizing different writing outcomes (rhetorical analysis, source engagement) and with different levels of success (as correlated to grades). \n",
        "\n",
        "In the remainder of this blog post, I'll discuss sections of the pipeline I've developed in Google Colab and how the work holds value for writing program instructors and administrators.\n",
        "\n"
      ],
      "metadata": {
        "id": "DSS87JYOvvFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages"
      ],
      "metadata": {
        "id": "zDd6JVT4cxbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIv9gPl0cBQo"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "#Install glob\n",
        "import glob \n",
        "\n",
        "#Install pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Install numpy\n",
        "import numpy as np\n",
        "\n",
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from  nltk.text import ConcordanceIndex\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Import matplotlib for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import logging  # Setting up the loggings to monitor gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Student Essays and Metadata"
      ],
      "metadata": {
        "id": "61xG-OZIdC0u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_EUmsB-u8i"
      },
      "source": [
        "###Import Student Essays and Add to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqzr4eGV2PYi"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OshZ_q2J-X85"
      },
      "outputs": [],
      "source": [
        "#Add files to upload from local machine\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aaq5zqob-ci5"
      },
      "outputs": [],
      "source": [
        "#Put essays into dataframe\n",
        "essays = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "\n",
        "#Reset index and add column names to make wrangling easier\n",
        "essays = essays.reset_index()\n",
        "essays.columns = [\"ID\", \"Text\"]\n",
        "\n",
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "essays['Text'] = essays['Text'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "#Remove newline characters and put in new column \n",
        "essays['Text_Newlines'] = essays['Text']\n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "essays.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add column without identifying information from each paper ID (instructor/student names) "
      ],
      "metadata": {
        "id": "Jm37ef7fdwUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNWKKEGI_IV2"
      },
      "outputs": [],
      "source": [
        "#Remove identifying information from ID\n",
        "#Remove any occurences of \"LATE_\" from dataset (otherwise will skew ID cleaning)\n",
        "essays['ID'] = essays['ID'].str.replace(r'LATE_', '', regex=True) \n",
        "\n",
        "#Split book on first underscore (_) in ID, keep only text in between first and second underscore (ID number)\n",
        "start = essays[\"ID\"].str.split(\"_\", expand = True)\n",
        "essays['ID'] = start[1]\n",
        "essays['ID'] = essays['ID'].astype(int)\n",
        "essays"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(essays)"
      ],
      "metadata": {
        "id": "otkAjDcw19n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I22itikk_QwV"
      },
      "source": [
        "### Import grades and additional metadata to second dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzgvrSb6_gnN"
      },
      "outputs": [],
      "source": [
        "#Upload csvs with essay metadata\n",
        "uploaded_grades = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntoCPrN2_xAt"
      },
      "outputs": [],
      "source": [
        "#Link to path where csv files are stored in drive\n",
        "local_path = r'/content'\n",
        "\n",
        "#Create variable to store all csvs in path\n",
        "filenames = glob.glob(local_path + \"/*.csv\")\n",
        "\n",
        "#Create df list for all csvs\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]\n",
        "\n",
        "len(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all data into one DataFrame\n",
        "metadata = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "#Change data to string (for further cleaning)\n",
        "metadata.astype(str)\n",
        "\n",
        "metadata.head()"
      ],
      "metadata": {
        "id": "uBIcqCFk2PQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgo-19g4_6AN"
      },
      "outputs": [],
      "source": [
        "#Drop header rows(Points Possible) and test student rows (Student, Test)\n",
        "metadata = metadata[metadata['Student'].str.contains('Points Possible|Student, Test')==False]\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep only relevant metadata (ID, Section, Final Portfolio Scores)\n",
        "clean_metadata = metadata[['ID'] + ['Section'] + list(metadata.loc[:, metadata.columns.str.startswith('Final Portfolio (')])]\n",
        "clean_metadata.head()\n",
        "#Want other metadata? Check the columns\n",
        "#Get all column names \n",
        "#for col in metadata.columns:\n",
        "   # print(col)"
      ],
      "metadata": {
        "id": "Et65_490s5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "180g-ohp_8Tq"
      },
      "outputs": [],
      "source": [
        "#Replace all NaN values with 0 \n",
        "clean_metadata = clean_metadata.replace(np.nan, 0)\n",
        "clean_metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzvBqD2__TA"
      },
      "outputs": [],
      "source": [
        "#Create new final portfolio column with all values\n",
        "#Add values of each column together; values except correct grade will be zero\n",
        "score_counts = clean_metadata.columns[2:]\n",
        "clean_metadata['Portfolio_Score'] = clean_metadata[score_counts].sum(axis=1)\n",
        "clean_metadata['Portfolio_Score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGJnhNdZAB9-"
      },
      "outputs": [],
      "source": [
        "#Drop grade columns for individual classes\n",
        "clean_metadata = clean_metadata[['ID', 'Section', \"Portfolio_Score\"]]\n",
        "clean_metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz_wEeL0ACdW"
      },
      "outputs": [],
      "source": [
        "#Drop decimal from ID (inconsistent with ID in essay dataframe)\n",
        "clean_metadata['ID'] = clean_metadata['ID'].astype(int)\n",
        "\n",
        "#Check cleaned DF one more time\n",
        "clean_metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMhaQbM2AEMC"
      },
      "source": [
        "### Merge essays and grade metadata into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwAjifpvALsP"
      },
      "outputs": [],
      "source": [
        "#Merge metadata and cleaned essays into new dataframe\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "essays_grades_master = clean_metadata.merge(essays,on='ID')\n",
        "\n",
        "#Print dataframe\n",
        "essays_grades_master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort dataframe by grades\n",
        "essays_grades_master.sort_values(by=['Portfolio_Score'], inplace = True)\n",
        "essays_grades_master"
      ],
      "metadata": {
        "id": "eLM2Jz5Gd05R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYmo80TOAPHf"
      },
      "outputs": [],
      "source": [
        "#Save new df to csv and download\n",
        "essays_grades_master.to_csv('essays_grades_master.csv') \n",
        "files.download('essays_grades_master.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save and download de-identified essays for future analysis\n",
        "#Add each text to a new list called paragraph_context\n",
        "deidentified_texts = []\n",
        "for row in essays_grades_master['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    deidentified_texts.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in essays_grades_master['ID'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir deidentified_texts\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in deidentified_texts:\n",
        "  f = open(\"deidentified_texts/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "\n",
        "#Zip text files in folder\n",
        "!zip -r deidentified_texts.zip deidentified_texts\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('deidentified_texts.zip')"
      ],
      "metadata": {
        "id": "bQUaYD_Zop3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clean Data"
      ],
      "metadata": {
        "id": "qIYZO-LW2sGO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZQOgu0Bi15"
      },
      "source": [
        "### Basic Cleaning with NLTK\n",
        "####Lowercasing, Punctuation Removal, and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Ux-p8QAvrZ"
      },
      "outputs": [],
      "source": [
        "#Rename dataframe\n",
        "clean_essay_grades_df = essays_grades_master\n",
        "clean_essay_grades_df.rename(columns = {\"Text_NoHeaders\": \"Text\"}, inplace = True)\n",
        "\n",
        "#Lowercase all words\n",
        "clean_essay_grades_df['Lower_Text'] = clean_essay_grades_df['Text'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['Lower_Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "clean_essay_grades_df['NoPunct_Text'] = clean_essay_grades_df['NoPunct_Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "clean_essay_grades_df['NoStops_Text'] = clean_essay_grades_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Check output\n",
        "clean_essay_grades_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Text into Paragraphs"
      ],
      "metadata": {
        "id": "3U1COuZkFQqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only need one newlines version here\n",
        "paragraphs_df = clean_essay_grades_df[['Portfolio_Score','ID', 'Text_Newlines']].copy()\n",
        "\n",
        "#Add ID and score in one column\n",
        "paragraphs_df['Score_ID'] = 'Score: ' + paragraphs_df['Portfolio_Score'].astype(str) + ', ID: ' + paragraphs_df['ID'].astype(str)\n",
        "\n",
        "#Check new df\n",
        "paragraphs_df.head()\n"
      ],
      "metadata": {
        "id": "PjxeIWG2HsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of paragraphs in each text\n",
        "paragraph_counts = paragraphs_df['Text_Newlines'].str.count(r'\\n')\n",
        "paragraph_counts\n",
        "\n",
        "#Append paragraphs counts to dataframe\n",
        "paragraphs_df[\"Paragraph_Counts\"] = paragraph_counts\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "AKA5Xxo7Dovo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new paragraph starts \n",
        "new = paragraphs_df[\"Text_Newlines\"].str.split(r'\\n', expand = True).set_index(paragraphs_df['Score_ID'])\n",
        "\n",
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "paragraphs_df = new.stack().reset_index()\n",
        "paragraphs_df.columns = [\"Score_ID\", \"Paragraph\", \"Text\"]\n",
        "\n",
        "#Split score and ID back to own columns\n",
        "paragraphs_df[['Score','ID']] = paragraphs_df.Score_ID.str.split(\", \",expand=True)\n",
        "paragraphs_df['Score'] = paragraphs_df['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "paragraphs_df['ID'] = paragraphs_df['ID'].map(lambda x: x.lstrip('ID: '))\n",
        "paragraphs_df['ID_Paragraph'] = paragraphs_df['ID'].astype(str) + '_' + paragraphs_df['Paragraph'].astype(str)\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "m-B7rbmwGv34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Clean paragraphs\n",
        "##Filter out paragraphs with 5 or less words (headers)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.split().str.len().lt(10)]\n",
        "\n",
        "## Filter out paragraphs containing \"http://\", \"doi:\" , \"https://\" and \"://www\" (Works Cited citations)\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"http://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"https://\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"://www\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"www.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\".com/\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"Vol.\")]\n",
        "\n",
        "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"doi:\")]\n",
        "\n",
        "paragraphs_df"
      ],
      "metadata": {
        "id": "rTz6Y4cyGv6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download to clean further\n",
        "#paragraphs_df.to_csv('paragraphs.csv') \n",
        "#files.download('paragraphs.csv')"
      ],
      "metadata": {
        "id": "1RuBKJo9Gv8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each paragraph as a txt file\n",
        "#Add each text to a new list called paragraphs\n",
        "paragraphs = []\n",
        "for row in paragraphs_df['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in paragraphs_df['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in paragraphs:\n",
        "  f = open(\"paragraphs/\" + filenames[n] +  '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r paragraphs.zip paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('paragraphs.zip')"
      ],
      "metadata": {
        "id": "ymIHF5KuF2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Identify Keywords in Context"
      ],
      "metadata": {
        "id": "ahLh1N1pB12b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outcome 1: Extracting Rhetorical Analysis Terms and Context"
      ],
      "metadata": {
        "id": "zTx8vs00B6bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Set up new dataframe for keyword frequency counts\n",
        "rhetorical_keywords_paragraphs_df = paragraphs_df.copy()\n",
        "\n",
        "#Count number of occurences of rhetorical terms in each paper\n",
        "pathos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('pathos')\n",
        "ethos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('ethos')\n",
        "logos_counts = rhetorical_keywords_paragraphs_df['Text'].str.count('logos')\n",
        "\n",
        "#Append each count to the dataframe\n",
        "rhetorical_keywords_paragraphs_df['Pathos_Counts'] = pathos_counts\n",
        "rhetorical_keywords_paragraphs_df[\"Ethos_Counts\"] = ethos_counts\n",
        "rhetorical_keywords_paragraphs_df[\"Logos_Counts\"] = logos_counts\n",
        "\n",
        "#Get summ of all term usages\n",
        "rhetorical_terms = ['Pathos_Counts', 'Ethos_Counts', 'Logos_Counts']\n",
        "rhetorical_keywords_paragraphs_df['Sum_Terms'] = rhetorical_keywords_paragraphs_df[rhetorical_terms].sum(axis=1)\n",
        "\n",
        "#Split score and ID back to own columns\n",
        "rhetorical_keywords_paragraphs_df[['Score','ID']] = rhetorical_keywords_paragraphs_df.Score_ID.str.split(\", \",expand=True)\n",
        "rhetorical_keywords_paragraphs_df['Score'] = rhetorical_keywords_paragraphs_df['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "rhetorical_keywords_paragraphs_df['ID'] = rhetorical_keywords_paragraphs_df['Score'].map(lambda x: x.lstrip('ID: '))\n",
        "\n",
        "rhetorical_keywords_paragraphs_df"
      ],
      "metadata": {
        "id": "4V7oWhKpSNQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all rows with no rhetorical terms\n",
        "rhetorical_keywords_paragraphs_df_no_blanks = rhetorical_keywords_paragraphs_df[rhetorical_keywords_paragraphs_df.Sum_Terms > 0]\n",
        "rhetorical_keywords_paragraphs_df_no_blanks"
      ],
      "metadata": {
        "id": "lXOWxEyISjkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "rhetorical_keywords_paragraphs_df_no_blanks.to_csv('rhetorical_keywords_paragraphs_df_no_blanks.csv') \n",
        "files.download('rhetorical_keywords_paragraphs_df_no_blanks.csv')"
      ],
      "metadata": {
        "id": "RLCLHX0wWO-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each rhetorical analysis paragraph as a txt file\n",
        "#Add each text to a new list called rhetorical_paragraphs\n",
        "rhetorical_paragraphs = []\n",
        "for row in rhetorical_keywords_paragraphs_df_no_blanks['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    rhetorical_paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in rhetorical_keywords_paragraphs_df_no_blanks['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir rhetorical_paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in rhetorical_paragraphs:\n",
        "  f = open(\"rhetorical_paragraphs/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r rhetorical_paragraphs.zip rhetorical_paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('rhetorical_paragraphs.zip')"
      ],
      "metadata": {
        "id": "hvFp5DsNCnIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this text mining, we have two new data sets to analyze: \n",
        "\n",
        "\n",
        "*   `rhetorical_keywords_paragraphs_df_no_blanks.csv`: A CSV file containing each paragraph where rhetorical terminology was used, along with relevant metadata (can be used for close-reading, frequency and regression analysis, PCA)\n",
        "*  `rhetorical_paragraphs.zip`: A zip file containing plain txt versions of each paragraph where rhetorical terminology was used (can be used for close-reading, DocuScope analysis, topic modeling, and/or other types of corpus analysis)\n",
        "\n"
      ],
      "metadata": {
        "id": "_W58h_OIDelH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outcome 2: Extracting Citation Practices and Context"
      ],
      "metadata": {
        "id": "inP4XLiwEe_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get any text inside parentheticals and count of parentheticals and append to dataframe\n",
        "#https://stackoverflow.com/questions/24696715/regex-for-match-parentheses-in-python\n",
        "parentheticals = r'(?<=\\().*?(?=\\))'\n",
        "\n",
        "#Add new list for parenthetical citations\n",
        "parenthetical_matches = []\n",
        "parenthetical_counts = []\n",
        "\n",
        "#Find all occurences of parenthetical citations in each paragraph of each text\n",
        "citation_df = paragraphs_df.copy()\n",
        "for text in citation_df['Text']:\n",
        "  matches = re.findall(parentheticals, text)\n",
        "  parenthetical_matches.append(matches)\n",
        "  parenthetical_counts.append(len(matches))\n",
        "\n",
        "#Make new column counting all appearances of parentheticals\n",
        "citation_df[\"Parentheticals\"] = parenthetical_matches\n",
        "citation_df['Parenthetical_Counts'] = parenthetical_counts\n",
        "\n",
        "citation_df"
      ],
      "metadata": {
        "id": "UFhkaiB6EsL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all rows with no parenthetical terms\n",
        "citation_df_no_blanks = citation_df[citation_df.Parenthetical_Counts > 0]\n",
        "citation_df_no_blanks"
      ],
      "metadata": {
        "id": "aZ0LY8--Es0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save new df to csv and download\n",
        "citation_df.to_csv('citation_df_no_blanks.csv') \n",
        "files.download('citation_df_no_blanks.csv')"
      ],
      "metadata": {
        "id": "rPXlUVqBFpuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download each paragraph as a txt file\n",
        "#Add each text to a new list called paragraphs\n",
        "citation_paragraphs = []\n",
        "for row in citation_df_no_blanks['Text'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    citation_paragraphs.append(row_string)\n",
        "\n",
        "#Add filenames to list\n",
        "filenames = []\n",
        "for row in citation_df_no_blanks['ID_Paragraph'].items():\n",
        "    row_string = (str(row[1]))\n",
        "    filenames.append(row_string)\n",
        "\n",
        "filenames[1]\n",
        "\n",
        "#Make new directory to store text files\n",
        "!mkdir citation_paragraphs\n",
        "\n",
        "#Write texts to files\n",
        "n = 0\n",
        "for item in citation_paragraphs:\n",
        "  f = open(\"citation_paragraphs/\" + filenames[n] + '.txt','w')\n",
        "  n= n+1\n",
        "  f.write(item)\n",
        "  f.close()\n",
        "  \n",
        "#Zip text files in folder\n",
        "!zip -r citation_paragraphs.zip citation_paragraphs\n",
        "\n",
        "#Download file to zip folder to run through DocuScope\n",
        "files.download('citation_paragraphs.zip')"
      ],
      "metadata": {
        "id": "u5oQFdSdFvwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this text mining, we have two new data sets to analyze: \n",
        "\n",
        "*   `citation_df.csv`: A CSV file containing each paragraph where rhetorical terminology was used, along with relevant metadata (can be used for close-reading, frequency and regression analysis, PCA)\n",
        "*  `rhetorical_paragraphs.zip`: A zip file containing plain txt versions of each paragraph where rhetorical terminology was used (can be used for close-reading, DocuScope analysis, topic modeling, and/or other types of corpus analysis)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax9DhFPMGvYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Analyze Language Pattern Differences Between Outcomes\n",
        "\n",
        "This section uses Principal Component Analysis to determine whether students use distinct language patterns in paragraphs where citation practices are used vs. where rhetorical language is used. The language patterns measured are identified by DocuScope, a computational rhetorical analysis platform. "
      ],
      "metadata": {
        "id": "6RvoCM2dIA9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload and concat DocuScope dataframes for two types of paragraph and label whether they are citation or rhetorical analysis (some may be duplicates)\n",
        "#Upload rhetorical csv with DocuScope data\n",
        "rhet_clusters = files.upload()\n",
        "\n",
        "rhet_clusters_df = pd.read_csv(rhet_clusters)\n",
        "rhet_clusters_df"
      ],
      "metadata": {
        "id": "L6eKfLNJIUbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conduct PCA on these\n",
        "\n"
      ],
      "metadata": {
        "id": "kPkolwCjIUeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhA8l4G6IUi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBKCR3JwIUlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsWMsVFlIUnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oa6WSF3_IUqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analyze Language Pattern Differences Between Scores\n",
        "This section uses frequency plots, regression analysis and PCA to determine whether rhetorical analysis term usage and/or citation practice usage are good indicators of score. This first involves getting full counts of each type of term usage in each text(rather than just counts per paragraph)."
      ],
      "metadata": {
        "id": "WsRg7V5fHIvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rhetorical Terms Regression Analysis"
      ],
      "metadata": {
        "id": "WwmA5Kv1JfXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MbDKMScBxLD"
      },
      "outputs": [],
      "source": [
        "#We need the metadata and text with newlines here; we'll also take the nostops text for further count analysis\n",
        "rhetorical_keywords_df_full_texts = clean_essay_grades_df[['ID', 'Section', 'Portfolio_Score', 'Text_Newlines', 'NoStops_Text']].copy()\n",
        "\n",
        "#Add ID and score in one column\n",
        "rhetorical_keywords_df_full_texts['Score_ID'] = 'Score: ' + rhetorical_keywords_df_full_texts['Portfolio_Score'].astype(str) + ', ID:' + rhetorical_keywords_df_full_texts['ID'].astype(str)\n",
        "\n",
        "#Check new df\n",
        "rhetorical_keywords_df_full_texts.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count usage of each term in each essay\n",
        "pathos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('pathos')\n",
        "ethos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('ethos')\n",
        "logos_counts = rhetorical_keywords_df_full_texts['NoStops_Text'].str.count('logos')\n",
        "\n",
        "#Append each count to the dataframe\n",
        "rhetorical_keywords_df_full_texts['Pathos_Counts'] = pathos_counts\n",
        "rhetorical_keywords_df_full_texts[\"Ethos_Counts\"] = ethos_counts\n",
        "rhetorical_keywords_df_full_texts[\"Logos_Counts\"] = logos_counts\n",
        "\n",
        "#Get summ of all term usages\n",
        "rhetorical_terms = ['Pathos_Counts', 'Ethos_Counts', 'Logos_Counts']\n",
        "rhetorical_keywords_df_full_texts['Sum_Terms'] = rhetorical_keywords_df_full_texts[rhetorical_terms].sum(axis=1)\n",
        "\n",
        "rhetorical_keywords_df_full_texts"
      ],
      "metadata": {
        "id": "NQlnfgqefBdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times each term was used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Pathos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Pathos_Counts\"]),\n",
        "    go.Bar(name='Ethos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Ethos_Counts\"]),\n",
        "    go.Bar(name='Logos Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Logos_Counts\"]),\n",
        "    go.Bar(name='All Term Counts', x=rhetorical_keywords_df_full_texts[\"Score_ID\"], y=rhetorical_keywords_df_full_texts[\"Sum_Terms\"]),\n",
        "\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Each Rhetorical Term in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "gaFnyucvshQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Sum_Terms'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Sum Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Sum Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Total Rhetorical Terms is \" + str(r))"
      ],
      "metadata": {
        "id": "NkfyNWfby4XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of usages of pathos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Pathos_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Pathos Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Pathos Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Pathos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of logos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Logos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Logos is \" + str(r))\n",
        "\n",
        "\n",
        "#Check if amount of usages of ethos is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(rhetorical_keywords_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(rhetorical_keywords_df_full_texts['Ethos_Counts'])\n",
        "\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Ethos is \" + str(r))"
      ],
      "metadata": {
        "id": "RnHA4zt-shS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot # paragraphs in which terms were used vs. essay grade\n",
        "##In other words, do more successful writers use terms in multiple paragrpahs (indicating more coherence)?\n",
        "\n",
        "#Count number of paragraphs where terms used and append to new dataframe\n",
        "new_Series = rhetorical_keywords_paragraphs_df_no_blanks['Score_ID'].value_counts(ascending=True)\n",
        "df3 = pd.DataFrame(new_Series).reset_index()\n",
        "df3\n",
        "\n",
        "df3.rename(columns={\"index\": \"Score_ID\", \"Score_ID\": \"Paragraph_Counts\"}, errors=\"raise\", inplace=True)\n",
        "df3[['ID','Score']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "\n",
        "df3\n",
        "\n",
        "#Plot paragraph counts per paper\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Paragraph Counts', x=df3[\"Score_ID\"], y=df3[\"Paragraph_Counts\"]),\n",
        "\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Number of Paragraphs Where Rhetorical Terms Were Used')\n",
        "fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QqTjuT92QSls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3[['Score','ID']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "df3['Score'] = df3['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "df3 = df3[['Score','Paragraph_Counts']].copy()\n",
        "df3 = df3.apply(pd.to_numeric)\n",
        "df3"
      ],
      "metadata": {
        "id": "TLiA3AQcwXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of paragraph term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "#Check if amount of usages of all terms per paragraph is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "\n",
        "\n",
        "x = np.array(df3['Score'])\n",
        "y = np.array(df3['Paragraph_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Paragraph Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Paragrah Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Terms per Paragraph is \" + str(r))"
      ],
      "metadata": {
        "id": "yEE4JNRrQSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citation Practice Regression Analysis\n"
      ],
      "metadata": {
        "id": "2WYNa-yxJ0tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using FULL TEXTS Get any text inside parentheticals and count of parentheticals and append to dataframe\n",
        "#https://stackoverflow.com/questions/24696715/regex-for-match-parentheses-in-python\n",
        "parentheticals = r'(?<=\\().*?(?=\\))'\n",
        "\n",
        "parenthetical_matches = []\n",
        "parenthetical_counts = []\n",
        "\n",
        "citation_df_full_texts = clean_essay_grades_df[['ID', 'Section', 'Portfolio_Score','Text']].copy()\n",
        "for text in citation_df_full_texts['Text']:\n",
        "  matches = re.findall(parentheticals, text)\n",
        "  parenthetical_matches.append(matches)\n",
        "  parenthetical_counts.append(len(matches))\n",
        "\n",
        "citation_df_full_texts[\"Parentheticals\"] = parenthetical_matches\n",
        "citation_df_full_texts['Parenthetical_Counts'] = parenthetical_counts\n",
        "citation_df_full_texts"
      ],
      "metadata": {
        "id": "Flph8YEZb-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add ID and score in one column\n",
        "citation_df_full_texts['Score_ID'] = 'Score: ' + citation_df_full_texts['Portfolio_Score'].astype(str) + ', ID:' + citation_df_full_texts['ID'].astype(str)"
      ],
      "metadata": {
        "id": "JQnqKx6BZkql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart number of times parentheticals were used in each essay \n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Parenthetical_Tags', x=citation_df_full_texts[\"Score_ID\"], y=citation_df_full_texts[\"Parenthetical_Counts\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Parentheticals Used in Each Essay')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "KDOgeBIvd4bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regression: Parentheticals vs. Grade\n",
        "\n",
        "#Check if amount of all term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "x = np.array(citation_df_full_texts['Portfolio_Score'])\n",
        "y = np.array(citation_df_full_texts['Parenthetical_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Parenthetical Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Parenthetical Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Parentheticals is \" + str(r))\n"
      ],
      "metadata": {
        "id": "t17Lgbdcd4dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot # paragraphs in which terms were used vs. essay grade\n",
        "##In other words, do more successful writers use terms in multiple paragrpahs (indicating more coherence)?\n",
        "\n",
        "#Count number of paragraphs where terms used and append to new dataframe\n",
        "new_Series = citation_df['Score_ID'].value_counts(ascending=True)\n",
        "df3 = pd.DataFrame(new_Series).reset_index()\n",
        "df3\n",
        "\n",
        "df3.rename(columns={\"index\": \"Score_ID\", \"Score_ID\": \"Paragraph_Counts\"}, errors=\"raise\", inplace=True)\n",
        "df3[['ID','Score']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "\n",
        "df3\n",
        "\n",
        "#Plot paragraph counts per paper\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Paragraph Counts', x=df3[\"Score_ID\"], y=df3[\"Paragraph_Counts\"]),\n",
        "\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Number of Paragraphs Where Citation Terms Were Used')\n",
        "fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FdDgTs-mKtAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3[['Score','ID']] = df3.Score_ID.str.split(\", \",expand=True)\n",
        "df3['Score'] = df3['Score'].map(lambda x: x.lstrip('Score: '))\n",
        "df3 = df3[['Score','Paragraph_Counts']].copy()\n",
        "df3 = df3.apply(pd.to_numeric)\n",
        "df3"
      ],
      "metadata": {
        "id": "krqsS0OfX40t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if amount of paragraph term usage is indicative of grade\n",
        "#Based on results (r = .08, there is little relationship between amount of rhetorical terms used and grade...at least between A and B range essays)\n",
        "from scipy import stats\n",
        "\n",
        "#Check if amount of usages of all terms per paragraph is indicative of grade\n",
        "#Create arrays of independent (x) and dependent (y) variables\n",
        "\n",
        "\n",
        "x = np.array(df3['Score'])\n",
        "y = np.array(df3['Paragraph_Counts'])\n",
        "\n",
        "#Return key values of linear regression\n",
        "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
        "\n",
        "#Create function to return new equation\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, mymodel)\n",
        "plt.title(\"Paragraph Counts By Score\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Paragrah Counts\")\n",
        "plt.show()\n",
        "\n",
        "print(\"R value for Citation Terms per Paragraph is \" + str(r))"
      ],
      "metadata": {
        "id": "yENb8z6OKxNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis for Rhetorical & Citation Terms"
      ],
      "metadata": {
        "id": "7DpvEhDDKek6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop unneeded columns from each dataframe to merge\n",
        "#DataFrame with all rhetorical term counts\n",
        "rhetorical_keywords_df_full_texts_merge = rhetorical_keywords_df_full_texts.drop(['Score_ID', 'Text_Newlines', 'NoStops_Text', \"Section\"], axis=1)\n",
        "\n",
        "#DataFrame with parenthetical citations\n",
        "citation_df_full_texts_merge = citation_df_full_texts.drop(['Score_ID', 'Text', 'Parentheticals', 'Portfolio_Score'], axis=1)\n",
        "\n",
        "#Merge DataFrames together\n",
        "dfs = [rhetorical_keywords_df_full_texts_merge, citation_df_full_texts_merge]\n",
        "\n",
        "import functools as ft\n",
        "df_final = ft.reduce(lambda left, right: pd.merge(left, right, on='ID'), dfs)\n",
        "\n",
        "df_final"
      ],
      "metadata": {
        "id": "XSgo_tAgiAka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get identifying data (scores and filenames)\n",
        "targets = df_final[['ID', 'Portfolio_Score']]\n",
        "\n",
        "#Sort by grade\n",
        "targets.sort_values(by=['Portfolio_Score'], inplace=True)\n",
        "\n",
        "#Create new column to label essays based on grade range\n",
        "def calc_new_col(row):\n",
        "  if row['Portfolio_Score'] <= 80:\n",
        "     return 'Low-Scoring'\n",
        "  if row['Portfolio_Score'] >= 95:\n",
        "     return 'High-Scoring'\n",
        "  else:\n",
        "    return 'Mid-Scoring'\n",
        "\n",
        "targets[\"Range\"] = targets.apply(calc_new_col, axis=1)\n",
        "\n",
        "#Drop unnecessary columns\n",
        "targets = targets.drop([\"Portfolio_Score\"], axis=1)\n",
        "\n",
        "targets.head()"
      ],
      "metadata": {
        "id": "tJhuOLICkfU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge target and df_final based ono filename\n",
        "PCA_df = pd.merge(targets, df_final, on='ID')\n",
        "\n",
        "PCA_df\n",
        "\n",
        "#Drop unnecessary columns\n",
        "PCA_df = PCA_df.drop([\"ID\",\"Section\", \"Portfolio_Score\"], axis=1)\n",
        "\n",
        "# shift column 'Range' to first position\n",
        "first_column = PCA_df.pop('Range')\n",
        "  \n",
        "# insert column using insert(position,column_name,\n",
        "# first_column) function\n",
        "PCA_df.insert(0, 'Range', first_column)\n",
        "PCA_df.head()\n"
      ],
      "metadata": {
        "id": "2T_jKD9WfR40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate the features from the target\n",
        "# Separating out the target\n",
        "y = PCA_df.loc[:,['Range']].values\n",
        "\n",
        "# Separating out the features\n",
        "PCA_df_2 = PCA_df.drop([\"Range\"], axis=1)\n",
        "x = PCA_df_2.values\n",
        "\n",
        "# Standardizing the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "x = StandardScaler().fit_transform(x)"
      ],
      "metadata": {
        "id": "7We30zjKhbOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Project to 2D via principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(x)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf"
      ],
      "metadata": {
        "id": "IiEKN5MafR9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenate with the range values\n",
        "finalDf = pd.concat([principalDf, PCA_df[['Range']]], axis = 1)"
      ],
      "metadata": {
        "id": "CwF8DctFfSZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot PCA results\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['Low-Scoring', 'Mid-Scoring','High-Scoring']\n",
        "colors = ['r', 'g', 'b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['Range'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "-B-dieMSmAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the explained variance ratio\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "LHYbRDW9mEB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}